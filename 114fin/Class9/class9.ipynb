{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d590b476",
   "metadata": {},
   "source": [
    "### Classification Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate a synthetic dataset with 356 rows and 8 features\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=356, n_features=8, \n",
    "                         n_informative=6, n_redundant=2, \n",
    "                         n_classes=2, random_state=42)\n",
    "\n",
    "# Convert to DataFrame (optional, for better visualization)\n",
    "df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(8)])\n",
    "df['Target'] = y\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Prepare features and target\n",
    "X = df.iloc[:, :-1]  # First 8 columns (features)\n",
    "y = df['Target']     # Last column (target)\n",
    "\n",
    "# Step 3: Split the data into training and testing sets (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 4: Standardize the features (important for KNN)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b577e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train the KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 5  # Number of neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Feature importance using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "perm_importance = permutation_importance(knn, X_train_scaled, y_train, n_repeats=10, random_state=42)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "significant_features = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Permutation Importance):\")\n",
    "print(significant_features)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "y_pred = knn.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c565a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix (Training)\n",
    "y_train_pred = knn.predict(X_train_scaled)\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Training)')\n",
    "plt.show()\n",
    "\n",
    "# Training Accuracy and AUC\n",
    "y_train_pred_prob = knn.predict_proba(X_train_scaled)[:, 1]\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training AUC: {train_auc:.4f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred_prob = knn.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred = knn.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb76b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve (Training and Test)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_prob)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_prob)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_train, tpr_train, label=f'Training (AUC = {train_auc:.4f})')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test (AUC = {test_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47200ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Try different values of k to find the best one\n",
    "print(\"\\n--- Testing different values of k ---\")\n",
    "k_range = range(1, 11)\n",
    "scores = {}\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_k = knn.predict(X_test_scaled)\n",
    "    scores[k] = accuracy_score(y_test, y_pred_k)\n",
    "    print(f\"k={k}, Accuracy={scores[k]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f45292",
   "metadata": {},
   "source": [
    "### Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe02c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# create a sample dataset \n",
    "from sklearn.datasets import make_regression\n",
    "X,Y = make_regression(n_features=1, noise=5, n_samples=5000)\n",
    "\n",
    "plt.xlabel('Feature - X')\n",
    "plt.ylabel('Target - Y')\n",
    "plt.scatter(X,Y,s=5)\n",
    "\n",
    "# Build the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X,Y)\n",
    "\n",
    "linear_model.coef_\n",
    "linear_model.intercept_\n",
    "\n",
    "# prediction\n",
    "pred = linear_model.predict(X)\n",
    "plt.scatter(X,Y,s=25, label='training')\n",
    "plt.scatter(X,pred,s=25, label='prediction')\n",
    "plt.xlabel('Feature - X')\n",
    "plt.ylabel('Target - Y')\n",
    "plt.legend()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19629f0",
   "metadata": {},
   "source": [
    "### Clustering - K-means Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# -----------------------------------------------\n",
    "# Step 1: Use the same dataset (5600 rows, 7 columns)\n",
    "# -----------------------------------------------\n",
    "from sklearn.datasets import make_blobs\n",
    "X, _ = make_blobs(n_samples=5600, centers=5, n_features=7, cluster_std=2.0, random_state=42)\n",
    "\n",
    "# Convert to DataFrame (we'll assume df from previous step, or recreate)\n",
    "df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(7)])\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 2: Standardize the data\n",
    "# -----------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "print(\"\\nData has been standardized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08486674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 3: Find the best k using Elbow Method and Silhouette Analysis\n",
    "# -----------------------------------------------\n",
    "from sklearn.metrics import silhouette_score\n",
    "# Range of k values to test\n",
    "k_range = range(2, 11)  # Test k from 2 to 10\n",
    "\n",
    "# Lists to store results\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"\\nEvaluating K-Means for k from 2 to 10...\")\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42, max_iter=300)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)  # WCSS (Within-cluster sum of squares)\n",
    "    \n",
    "    # Silhouette score (slower for large data — sample if needed)\n",
    "    if k > 1:\n",
    "        # Use a sample for silhouette to save time (optional)\n",
    "        sample_size = 1000\n",
    "        indices = np.random.choice(X_scaled.shape[0], size=sample_size, replace=False)\n",
    "        X_sample = X_scaled[indices]\n",
    "        score = silhouette_score(X_sample, kmeans.labels_[indices])\n",
    "        silhouette_scores.append(score)\n",
    "    else:\n",
    "        silhouette_scores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aed729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 4: Plot Elbow and Silhouette\n",
    "# -----------------------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Method\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=6)\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Within-cluster Sum of Squares (WCSS)')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette Analysis\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=6)\n",
    "ax2.set_title('Silhouette Score vs k')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 5: Choose best k\n",
    "# -----------------------------------------------\n",
    "# Find k with highest silhouette score\n",
    "best_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nBest k based on Silhouette Score: {best_k_silhouette}\")\n",
    "\n",
    "# Optional: Elbow \"knee\" detection (manual or use kneed library)\n",
    "# For this example, we'll go with silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cfeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Step 6: Apply K-Means with best k and add labels to df\n",
    "# -----------------------------------------------\n",
    "final_k = best_k_silhouette  # or set to 5 if you know it from data\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=final_k, init='k-means++', n_init=10, random_state=42)\n",
    "cluster_labels_kmeans = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add K-Means cluster labels to df\n",
    "df['KMeans_Cluster'] = cluster_labels_kmeans\n",
    "\n",
    "print(f\"\\nK-Means clustering completed with k = {final_k}\")\n",
    "print(\"K-Means Cluster distribution:\")\n",
    "print(df['KMeans_Cluster'].value_counts().sort_index())\n",
    "\n",
    "# Optional: Show first 10 rows with K-Means labels\n",
    "print(\"\\nFirst 10 rows with K-Means cluster labels:\")\n",
    "print(df[['KMeans_Cluster'] + [f'Feature_{i+1}' for i in range(7)]].head(10))\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 7: (Optional) Visualize clusters using PCA\n",
    "# -----------------------------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1],\n",
    "    hue=df['KMeans_Cluster'],\n",
    "    palette='Set1',\n",
    "    s=50,\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(f'K-Means Clustering Results (k={final_k}) - PCA Projection')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Final Output\n",
    "# -----------------------------------------------\n",
    "print(f\"\\n Final DataFrame shape: {df.shape}\")\n",
    "print(\"Each row now has a K-Means cluster label in the 'KMeans_Cluster' column.\")\n",
    "print(\"Access labels: df['KMeans_Cluster']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9161de",
   "metadata": {},
   "source": [
    "## Aditional Information\n",
    "- Review the backtesting\n",
    "- Position Sizing Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca32d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "# -------------------------------\n",
    "# 1. Download AAPL Data\n",
    "# -------------------------------\n",
    "ticker = \"AAPL\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2030-12-31\"\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651550e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Calculate technical indicators\n",
    "# -------------------------------\n",
    "# Simple Moving Averages\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['MA_50'] = data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# RSI\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.clip(lower=0)\n",
    "loss = -delta.clip(upper=0)\n",
    "avg_gain = gain.rolling(window=14).mean()\n",
    "avg_loss = loss.rolling(window=14).mean()\n",
    "rs = avg_gain / avg_loss.replace(0, np.nan)  # Avoid division by zero\n",
    "data['RSI'] = 100 - (100 / (1 + rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Define trading strategy (MA Crossover + RSI Filter)\n",
    "# -------------------------------\n",
    "data['Buy_Signal'] = (data['MA_20'] > data['MA_50']) & (data['MA_20'].shift(1) <= data['MA_50'].shift(1)) & (data['RSI'] < 60)\n",
    "data['Sell_Signal'] = (data['MA_20'] < data['MA_50']) & (data['MA_20'].shift(1) >= data['MA_50'].shift(1))\n",
    "\n",
    "# Fill NaN values in signals to False to ensure valid booleans\n",
    "data['Buy_Signal'] = data['Buy_Signal'].fillna(False)\n",
    "data['Sell_Signal'] = data['Sell_Signal'].fillna(False)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Backtest strategy\n",
    "# -------------------------------\n",
    "initial_capital = 100_000\n",
    "cash = initial_capital\n",
    "shares = 0\n",
    "position = 0  # 0 = no position, 1 = holding\n",
    "portfolio_value = []\n",
    "\n",
    "for date in data.index[1:]:\n",
    "    open_price = data.loc[date, 'Close'].item()  # Use .item() for scalar\n",
    "    prev_date = data.index[data.index.get_loc(date) - 1]  # Get previous date\n",
    "    if data.loc[prev_date, 'Buy_Signal'].item() and position == 0:\n",
    "        shares = cash // open_price\n",
    "        cash -= shares * open_price\n",
    "        position = 1\n",
    "    elif data.loc[prev_date, 'Sell_Signal'].item() and position == 1:\n",
    "        cash += shares * open_price\n",
    "        shares = 0\n",
    "        position = 0\n",
    "    portfolio_value.append(float(cash + shares * data.loc[date, 'Close']))  # Convert to float\n",
    "\n",
    "portfolio_df = pd.DataFrame(portfolio_value, index=data.index[1:], columns=['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc279d8e",
   "metadata": {},
   "source": [
    "### Buy-signal: all buy no cash, Sell-signal:all sell get cash? - Position sizing!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a636db",
   "metadata": {},
   "source": [
    "### Position Sizing Management\n",
    "\n",
    "**Position sizing** means deciding *how many shares (or contracts) to buy* when entering a trade. It's not about *whether* to trade, but *how big* the trade should be. Good position sizing helps manage risk — so you don’t lose too much on a single bad trade.\n",
    "\n",
    "Three approaches we’ll implement are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef9364",
   "metadata": {},
   "source": [
    "1. **\"Position sizing: Allocate 50% of current portfolio value\"** - is not risk-based one.\n",
    "\n",
    "- It’s a way to decide **how much** to invest in a single trade. Instead of risking a fixed dollar amount or using volatility (like ATR), you're simply committing **half of your portfolio** to this trade.\n",
    "\n",
    "- Your **portfolio is worth $100,000** (all in cash for now), You get a buy signal, The stock price is \\$50 per share. So you buy **1,000 shares** for \\$50,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "initial_capital = 100_000\n",
    "cash = initial_capital\n",
    "shares = 0\n",
    "position = 0  # 0 = no position, 1 = holding\n",
    "portfolio_value = []\n",
    "position_fraction = 0.5  # 50% of portfolio per trade\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    date = data.index[i]\n",
    "    prev_date = data.index[i-1]\n",
    "    close_price = data.loc[date, 'Close'].item()\n",
    "    current_value = cash + (shares * close_price) if position == 1 else cash\n",
    "    if data.loc[prev_date, 'Buy_Signal'].item():\n",
    "        if position == 0:\n",
    "            # Position sizing: Allocate 50% of current portfolio value\n",
    "            target_value = current_value * position_fraction\n",
    "            shares = int(target_value / close_price)  # Integer shares\n",
    "            if shares > 0 and cash >= shares * close_price:\n",
    "                cash -= shares * close_price\n",
    "                position = 1\n",
    "    elif data.loc[prev_date, 'Sell_Signal'].item() and position == 1:\n",
    "        cash += shares * close_price\n",
    "        shares = 0\n",
    "        position = 0\n",
    "    portfolio_value.append(float(cash + shares * close_price))\n",
    "\n",
    "portfolio_df = pd.DataFrame(portfolio_value, index=data.index[1:], columns=['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f862c3",
   "metadata": {},
   "source": [
    "2. Risk-Based Position Sizing: Position sizing: Risk 1% of portfolio, using ATR as risk per share\n",
    "\n",
    "   -  What does **\"Risk 1% of portfolio\"** mean?\n",
    "\n",
    "For example: If your portfolio is worth \\$100,000 → 1% = \\$1,000. So, you size your trade so that if it fails (i.e., price moves against you), you lose **at most \\$1,000**. This is called **percent risk model** and is widely used by professional traders.\n",
    "\n",
    "- Risk 1% of the portfolio per trade, adjusting the number of shares based on the stock’s volatility or stop-loss distance.\n",
    "\n",
    "-  What is **ATR**? - risk!\n",
    "\n",
    "**ATR = Average True Range**, a technical indicator that measures market volatility. Here, ATR is used as a **proxy for risk per share**. So if ATR = \\$5, it means the stock typically moves around \\$5 per day — so you might assume your stop-loss could be about that far. \n",
    "\n",
    "What is the **Risk 1% of portfolio, using ATR as risk per share**? \n",
    "\n",
    "We want to buy enough shares so that if the price drops by **one ATR**, we lose **exactly 1% of our portfolio**.\n",
    "\n",
    "- $$\n",
    "  \\text{Position Size(Shares)}=\\frac{\\text{Portfolio Value}\\times \\text{Risk per Trade}}{\\text{Risk per Share}}\n",
    "  $$\n",
    "\n",
    "  where Risk Per Share is often the difference between the entry price and stop-loss or a volatility measure. We need to estimate **how much each share could risk** (≈ ATR), then to decide **how much total money you're willing to risk** (1% of portfolio)\n",
    "\n",
    "Assume: Current portfolio value = \\$100,000, You risk 1% → \\$1,000. and ATR = \\$10 → you assume price might move \\$10 against you (your \"risk per share\") --> share = 1000/10 =100 shares.  If the price drops by \\$10 (1 ATR), you lose: 100 shares × \\$10 = $1,000 → exactly 1% of portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd90998",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The ATR is proxy as the risk per trade\n",
    "data['TR'] = data['Close'].diff().abs()  \n",
    "               # True Range approximated as |Close - Previous Close|\n",
    "data['ATR'] = data['TR'].rolling(window=14).mean()  # 14-day moving average\n",
    "\n",
    "######\n",
    "initial_capital = 100_000\n",
    "cash = initial_capital\n",
    "shares = 0\n",
    "position = 0  # 0 = no position, 1 = holding\n",
    "portfolio_value = []\n",
    "risk_per_trade = 0.01  # Risk 1% of portfolio per trade\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    date = data.index[i]\n",
    "    prev_date = data.index[i-1]\n",
    "    close_price = data.loc[date, 'Close'].item()\n",
    "    atr_value = data.loc[date, 'ATR'].item()  # Extract scalar ATR\n",
    "    atr = atr_value if not pd.isna(atr_value) else 1.0  \n",
    "                        # Fallback to 1.0 if ATR is NaN\n",
    "    current_value = cash + (shares * close_price) if position == 1 else cash\n",
    "    if data.loc[prev_date, 'Buy_Signal'].item():\n",
    "        if position == 0:\n",
    "            # Position sizing: Risk 1% of portfolio, using ATR as risk per share\n",
    "            risk_amount = current_value * risk_per_trade\n",
    "            shares = int(risk_amount / atr) if atr > 0 else 0  \n",
    "                              # Integer shares based on ATR\n",
    "            if shares > 0 and cash >= shares * close_price:\n",
    "                cash -= shares * close_price\n",
    "                position = 1\n",
    "    elif data.loc[prev_date, 'Sell_Signal'].item() and position == 1:\n",
    "        cash += shares * close_price\n",
    "        shares = 0\n",
    "        position = 0\n",
    "    portfolio_value.append(float(cash + shares * close_price))\n",
    "\n",
    "portfolio_df = pd.DataFrame(portfolio_value, index=data.index[1:], columns=['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5d898",
   "metadata": {},
   "source": [
    "The **True Range (TR)** is the maximum of:\n",
    "\n",
    "- High price minus low price. \n",
    "- Absolute difference between high price and previous close.\n",
    "- Absolute difference between low price and previous close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcba3f",
   "metadata": {},
   "source": [
    "3. **Kelly Criterion**\n",
    "\n",
    "- The last one is the **fixed fractional risk model** (risk 1% of portfolio per trade, using ATR to determine position size). Now you'd like to **replace or enhance** this with the **Kelly Criterion**.\n",
    "\n",
    "- Optimizes position size to maximize long-term portfolio growth based on the expected win rate and reward-to-risk ratio.\n",
    "\n",
    "  - $$\n",
    "    \\text{Kelly Fraction}=\\frac{\\text{Win Prob}-\\frac{1-\\text{win Prob}}{\\text{Reward to risk Ratio}}}{\\text{Reward to risk Ratio}}\n",
    "    $$\n",
    "\n",
    "  - The Kelly fraction f is the percentage of capital to allocate. To reduce risk, a **fractional Kelly** (e.g., 0.5 * Kelly) is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a00738",
   "metadata": {},
   "outputs": [],
   "source": [
    "### build the ATR\n",
    "data['TR'] = data['Close'].diff().abs()  \n",
    "               # True Range approximated as |Close - Previous Close|\n",
    "data['ATR'] = data['TR'].rolling(window=14).mean()  # 14-day moving average\n",
    "\n",
    "### backtesting\n",
    "initial_capital = 100_000\n",
    "cash = initial_capital\n",
    "shares = 0\n",
    "position = 0  # 0 = no position, 1 = holding\n",
    "portfolio_value = []\n",
    "\n",
    "# Kelly inputs — these should be estimated from backtesting!\n",
    "win_probability = 0.55        # p: 55% of trades are winners\n",
    "avg_win_to_risk_ratio = 2.0   # b: average win is 2x the risk (e.g., 2×ATR target)\n",
    "# So if you risk 1 ATR, you gain 2 ATR on average when you win\n",
    "\n",
    "# Optional: Use Fractional Kelly to reduce risk\n",
    "fractional_kelly = 0.5  # Use 50% of Kelly recommendation\n",
    "\n",
    "for i in range(1, len(data)):\n",
    "    date = data.index[i]\n",
    "    prev_date = data.index[i-1]\n",
    "    close_price = data.loc[date, 'Close'].item()\n",
    "    \n",
    "    atr_value = data.loc[date, 'ATR'].item()\n",
    "    atr = atr_value if not pd.isna(atr_value) else 1.0\n",
    "    \n",
    "    current_value = cash + (shares * close_price) if position == 1 else cash\n",
    "\n",
    "    if data.loc[prev_date, 'Buy_Signal'].item():\n",
    "        if position == 0:\n",
    "            # === Kelly Criterion: Optimal bet fraction ===\n",
    "            q = 1 - win_probability  # probability of loss\n",
    "            b = avg_win_to_risk_ratio\n",
    "            \n",
    "            kelly_fraction = (b * win_probability - q) / b  # Kelly formula\n",
    "            \n",
    "            # Apply fractional Kelly for safety\n",
    "            position_fraction = fractional_kelly * kelly_fraction\n",
    "            \n",
    "            # Don't go negative if edge is negative\n",
    "            if position_fraction <= 0:\n",
    "                print(f\"Kelly suggests no edge at {date}, skipping trade.\")\n",
    "                shares = 0\n",
    "            else:\n",
    "                # Allocate a % of portfolio based on Kelly\n",
    "                target_value = current_value * position_fraction\n",
    "                shares = int(target_value / close_price)\n",
    "                \n",
    "                # Check affordability\n",
    "                if shares > 0 and cash >= shares * close_price:\n",
    "                    cash -= shares * close_price\n",
    "                    position = 1\n",
    "                else:\n",
    "                    shares = 0  # Can't afford even one share\n",
    "\n",
    "    elif data.loc[prev_date, 'Sell_Signal'].item() and position == 1:\n",
    "        # Exit position\n",
    "        cash += shares * close_price\n",
    "        shares = 0\n",
    "        position = 0\n",
    "\n",
    "    # Record portfolio value\n",
    "    portfolio_value.append(float(cash + shares * close_price))\n",
    "\n",
    "# Build portfolio value DataFrame\n",
    "portfolio_df = pd.DataFrame(portfolio_value, index=data.index[1:], columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f8092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Calculate performance metrics\n",
    "# -------------------------------\n",
    "final_value = float(portfolio_value[-1])  # Ensure scalar\n",
    "days = (data.index[-1] - data.index[0]).days\n",
    "# cagr = ((final_value / initial_capital) ** (365.25 / days) - 1) if days > 0 else 0\n",
    "returns = portfolio_df['value'].pct_change().dropna()\n",
    "volatility = returns.std() * np.sqrt(252)\n",
    "# sharpe_ratio = (cagr - 0.03) / volatility if volatility != 0 else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{ticker} Trading Strategy Results\")\n",
    "print(f\"Period: {start_date} to {end_date}\")\n",
    "print(f\"Initial Capital: ${initial_capital:,.0f}\")\n",
    "print(f\"Final Value: ${final_value:,.0f}\")\n",
    "# print(f\"CAGR: {cagr:.2%}\")\n",
    "# print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cfa4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 6. Visualization\n",
    "# -------------------------------\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Plot price and signals\n",
    "ax1.plot(data.index, data['Close'], label='Close Price', color='black') # Close\n",
    "ax1.plot(data.index, data['MA_20'], label='MA 20', color='blue')        # MA_20\n",
    "ax1.plot(data.index, data['MA_50'], label='MA 50', color='red')         # MA_50\n",
    "\n",
    "ax1.scatter(data.index[data['Buy_Signal']], data['Close'][data['Buy_Signal']], marker='^', color='green', label='Buy') #up-- Taiwan red\n",
    "ax1.scatter(data.index[data['Sell_Signal']], data['Close'][data['Sell_Signal']], marker='v', color='red', label='Sell') # down-- Taiwan green\n",
    "\n",
    "ax1.set_title(f'{ticker} Price and Signals')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot portfolio value\n",
    "ax2.plot(portfolio_df.index, portfolio_df['value'], label='Portfolio Value', color='blue')\n",
    "ax2.set_title('Portfolio Value')\n",
    "ax2.set_ylabel('Value ($)')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
