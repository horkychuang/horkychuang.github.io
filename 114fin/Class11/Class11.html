<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 11</title>
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <!-- Load MathJax -->
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <!-- Load Marked.js for Markdown parsing -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* General Styling */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background: #f4f4f9; /* Light Gray Background */
            color: #333;
        }

        /* Navigation Bar at TOP*/
        nav {
            background-color: #3498db; /* Blue Background */
            color: white;
            padding: 10px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        nav h1 {
            margin: 0;
            font-size: 24px;
        }
        nav ul {
            list-style: none;
            margin: 0;
            padding: 0;
            display: flex;
            gap: 20px;
        }
        nav ul li {
            display: inline;
        }
        nav ul li a {
            color: white;
            text-decoration: none;
            font-size: 18px;
            transition: color 0.3s ease;
        }
        nav ul li a:hover {
            color: #ecf0f1; /* Lighter White on Hover */
        }

        /* Section Styling */
        section {
            width: 80%;
            max-width: 900px;
            margin: 50px auto;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            text-align: left;
        }
        h1, h2, h3 {
            color: #34495e;
        }
        p, li {
            font-size: 18px;
            line-height: 1.6;
            color: #555;
        }
        pre {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 14px;
        }
        code {
            color: #e74c3c;
        }

        /* 兩種div的定義：Summary and Discussion */
        .summary {
            background-color: #ecf0f1;
            padding: 15px;
            border-left: 5px solid #3498db;
            text-align: left;
        }
        .discussion {
            background-color: #fef9e7;
            padding: 15px;
            border-left: 5px solid #f1c40f;
            text-align: left;
        }
    </style>
</head>
<body>

<!-- Navigation Bar -->
<nav>
    <h1>Class 11 Deep Learning</h1>
    <ul> <!-- NAV BAR在上面, 要跟下面的大 section們有連接 , -->
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#topic1">Model</a></li>
        <li><a href="#topic2">Loss</a></li>
        <li><a href="#topic3">Optimization</a></li>
    </ul>
</nav>

<!-- Introduction Section -->
 <section id="Introduction">
    <div class="discussion">
    <h1>Today's Topic</h1>
    <ol>
        <li>Introduction</li>
        <li>DL Model</li>
        <li>OPtimization</li>
    </ol> 
</div>
</section>

<section id="introduction">
    <div class="summary">
    <h1>Introduction</h1>
    <p><small>
        The first example of an Neural Network is called the <mark>perceptron</mark>, 
        and this was invented by Frank Rosenblatt in 1957 The `*perceptron` is a classification algorithm. 
        The perceptron is an example of a simple one-layer neural feedforward network. 
        It has a vector of weights, w, and its output is a function, f(x ⋅ w), of the dot product, $x \cdot w$  of the weights and input. 
    </small></p>
    <img src="images\061101.jpg" alt="061101" width="300">
</div>
</section>


<section id="topic1">
<div class="summary">
    <h2>Neural Network</h2>
    <p><small>
       A classic multilayer perceptron has <mark>multiple</mark> interconnected perceptrons.
       An example of a neural network is shown in this figure. The left column is called the input layer; 
       the right column the output layer; and the middle column the middle layer. The middle layer is also known as a hidden layer.
       The units that are organized in different sequential layers (input layer, one or more hidden layers, and an output layer). 
       The following diagram depicts an neural network with one hidden layer. 
    </small></p>
    <img src="images\062517.jpg" alt="062517" width="300">
    <p><small>
       The functions that can be defined as follows
       <div style="text-align: center; margin: 20px 0;">
            $$
            y=f(\sum_{i=1}^n x_iw_i+b)\\
y=f(x\cdot w+b)
            $$
        </div>
        ,where The b is thus referred to as a bias while w is referred to as weight. 
        The f(x) function shown below is typically called an <mark>activation function</mark>. 
        The activation function has the following properties: `Non-linear` and `Differentiable`.
    </small></p>
</div>
</section>

<section>
<div class="summary">
    <h2>Multi-layer Neural Networks</h2>
    <p><small>
    These extra layers are called <mark>hidden layers</mark>. 
    The following diagram demonstrates a four-layer fully connected NN with two hidden layers
    </small></p>
    <img src="images\062519.jpg" alt="062519" width="300">  
</div>
</section>


<section>
<div class="discussion">
    <h1>Activation functions</h1>
    <p><small>
       <mark>Sigmoid</mark>: Its output is bounded between 0 and 1 and can be interpreted stochastically as the probability of the unit being active.<br>
       <img src="images/062520.jpg" alt="062520" width="300"> <br>
       <mark>Hyperbolic tangent (tanh)</mark>: The name speaks for itself. The principal difference with the sigmoid is that the tanh is in the (-1, 1) range.<br>
       <img src="images/062521.jpg" alt="062521" width="400"> <br>
       <mark>Rectified Linear Unit (ReLU)</mark>: the ReLU repeats its input when x > 0 and stays at 0 otherwise.
       <img src="images/062522.jpg" alt="062522" width="400"> <br>
    </small></p>
</div>
</section>

<section>
<div class="discussion">
    <p><small>
       <mark>Leaky ReLU</mark>: When $x < 0$, the leaky ReLU outputs x multiplied by some constant, $\alpha (0 < \alpha < 1)$, 
       instead of 0. The following diagram shows the leaky ReLU formula, its derivative, and their graphs for $\alpha = 0.2$:
       <img src="images/062523.jpg" alt="062523" width="400"> <br>
       <mark>Softmax</mark>: It is the activation function of the output layer in `classification problems`. 
       Let’s assume that the output of the final network layer is a vector, $z=(z_1, z_2,..., z_n)$. 
       Each of the n elements represents one of n classes, to which the input sample might belong. To determine the network prediction, 
       we’ll take the index, i, of the highest value, $z_i$ , and assign the input sample to the class it represents.
       <div style="text-align: center; margin: 20px 0;">
            $$
            f(z_i)=\frac{\text{exp}(z_i)}{\sum_{j=1}^n \text{exp}(z_j)}
            $$
       </div>
    </small></p>
    
</div>
</section>

<section >
<div class="summary">
    <h1>Train Neural Network</h1>
    <p><small>
       Given the model and dataset, we talk about training, we mean automatically acquiring optimum <mark>weight and b</mark> parameters from training data. 
       We shall use a method called `gradient method` to detect the least <mark>loss function</mark> value that utilizes the gradient of a function. 
    </small></p>
    <h2>Loss Function</h2>
    <p><small>
    The current state of the neural network training is represented by a score. 
    The score used in neural network training is called a loss function. 
    Even if a loss function is utilized, square mistakes or cross-entropy errors are usually summed up.
    <ul>
        <li>Sum of squared errors</li>
        <div style="text-align: center; margin: 20px 0;">
            $$
            E=\frac{1}{2}\sum_k (y_k -t_k)^2
            $$
        </div>
        , where $y_k$ is the result of the neural network and $t_k$ is the label for the data. 
        <li>Cross-entropy error</li> 
        <div style="text-align: center; margin: 20px 0;">
            $$
            E=-\sum_k t_k log(y_k)
            $$
        </div>
    </ul>
    </small></p>
</div>
</section>

<section >
<div class="summary">
    <h2>Backpropagation</h2>
    <p><small>
        Assumed that Taro purchased two oranges and three mangoes. 
        The cost of one orange was 100 yen and of one mango was 150 yen. 
        A tax of 10% was levied on consumption. 
        The amount of money that he paid may be calculated.
    </small></p>
    <img src="images\062001.jpg" alt="062001" width="400">
    <p><small>
        After creating a computational graph, we make the calculation from left to right. It is called <mark>forward propagation</mark>.<br>
        
        On the other hand, from right to left can also be taken into consideration. 
        This is termed backward propagation and is referred to as <mark>backpropagation</mark> (or backprop).<br>
        If we need to determine how the final amount paid is influenced by each of the three variables (prices of a mango, quantity of mangoes, and consumption tax). 
        That means we need to find the derivative of the amount paid for the mango’s prices, the derivative of the amount payable with respect to the number of mangoes, 
        and the derivative of the amount paid as far as consumption tax is concerned. 
    </small></p>
    <img src="images\062005.jpg" alt="062005" width="400">
</div>
</section>

<section>
<div class="discussion">
    <h2>Gradient method</h2>
    <p><small>
       The gradients here are those of a loss function for weight parameters. 
       For example, let’s assume that a neural network has the weight W (2x3 array) only, 
       and the loss function is L. In this case, we can express the gradient as $\frac{\partial L}{\partial W}$
    </small></p>
    <div style="text-align: center; margin: 20px 0;">
            $$
           W=\left( \begin{matrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{matrix} \right) \\

\frac{\partial L}{\partial W}=\left( \begin{matrix}
\frac{\partial L}{\partial w_{11}} & \frac{\partial L}{\partial w_{12}} & \frac{\partial L}{\partial w_{13}} \\
\frac{\partial L}{\partial w_{21}} & \frac{\partial L}{\partial w_{22}} & \frac{\partial L}{\partial w_{23}}
\end{matrix} \right)
            $$
    </div>
    <p><small>
        There are two popular gradient methods
        <ol>
            <li>Stochastic Gradient Descent(SGD)</li>
            <li>Adaptive Moment Estimation</li>
        </ol>
    </small></p>
</div>
</section>

<section >
<div class="summary">
    <h2>Types of Neural Networks</h2>
    <p><small>
        Over the years, different types have been developed to solve specific kinds of problems. 
        Below are the main types of neural networks used  today
    </small></p>
    <table border="1">
    <tr>
        <td>Data Type</td>
        <td>Network</td>
    </tr>
    <tr>
        <td>Tabular / Vector</td>
        <td>MLP (Multilayer Perceptron)</td>
    </tr>
    <tr>
        <td>Images / Grids</td>
        <td>CNN (Convolutional NN)</td>
    </tr>
    <tr>
        <td>Sequences / Text</td>
        <td>RNN → LSTM/GRU → Transformer</td>
    </tr>
    <tr>
        <td>Unsupervised / Encoding</td>
        <td>Autoencoder</td>
    </tr>
    <tr>
        <td>Generation</td>
        <td>GAN, VAE, Diffusion Models</td>
    </tr>
    <tr>
        <td>Attention-based</td>
        <td>Transformer</td>
    </tr>
</table>
</div>
</section>

<section >
<div class="summary">
    <h1>Pytorch and Keras version</h1>
    <p><small>
       Keras and PyTorch are two of the most popular tools for deep learning. 
       <mark>Keras</mark> is a high-level deep learning API designed for ease of use and rapid prototyping. 
       Originally developed by François Chollet, it now serves as the official high-level API for TensorFlow, Google’s deep learning framework. 
       <mark>PyTorch</mark> is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). 
       It emphasizes flexibility and dynamic computation. .
    </small></p>
    <p><small>
        <ol>
            <li>Start with Keras if you're new to deep learning or want to build models quickly since Keras: 
                Faster to write and debug for standard tasks.</li>
            <li>Switch to PyTorch when you need more control, want to do research, 
                or dive into advanced topics since better for custom loss functions, complex training logic, or debugging step-by-step</li>    
        </ol>
    </small></p>
</div>
</section>

<section id="topic2">
<div class="discussion">
    <pre><code>
# pip install tensorflow
import numpy as np
import tensorflow as tf 
from sklearn.preprocessing import LabelBinarizer

# Generate data
np.random.seed(42)  # For reproducibility
X_train = np.random.rand(1000, 100).astype(np.float32)  # 1000 samples, 100 features
y_train = np.random.randint(0, 10, size=(1000,))  # 10 classes
y_train = LabelBinarizer().fit_transform(y_train)  # One-hot encode labels

# Define a simple sequential model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(100,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
    </code></pre>
</div>
</section>


<section >
<div class="summary">
    <h1>Build a Model</h1>
    <p><small>
       After get the train data at Stage I, we build the model.
       <ol>
        <li>Model: sequence() is a **model class** in Keras that allows you to <mark>stack layers linearly</mark>. 
            Think of it as a “layer sandwich” — you stack layers from input to output.</li>
        <li>
            <pre><code>
model = tf.keras.Sequential([
Layer1,
Layer2,
Layer3,
...
])
            </code></pre>
        </li>
        <li>Dense() is a <mark>fully connected neural network layer</mark> 
            — every neuron in this layer connects to every neuron in the previous layer. 
            Dense layer in Keras combines the linear transformation (matrix multiplication and bias addition) 
            and the activation function into a single layer</li>
        <li>
            <pre><code>
Dense(
    units,                 # number of neurons in this layer
    activation=None,       # activation function (e.g., 'relu', 'softmax')
    input_shape=None,      # only needed for first layer
    use_bias=True,         # whether to add bias (default: True)
    kernel_initializer='glorot_uniform',  # weight initializer
    ...
)
# output = activation(dot(input, weights) + bias)                
            </code></pre>
        </li> 
        <ol>
            <li>tf.keras.layers.Dense: (output_dim, activation,  input_dim, **kwargs)</li>
            <li>input_dim or input_shape: input_shape(100, ) = input_dim(100)</li>
        </ol>   
       </ol>
    </small></p>
</div>
</section>

<section >
<div class="discussion">
    <pre><code>
# Build model
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),  # hidden layer 1
    Dense(64, activation='relu'),                      # hidden layer 2
    Dense(10, activation='softmax')                    # output layer
])
'''
Input: 784 features 
Hidden layer 1: 128 neurons with ReLU
Output layer: 10 neurons with Softmax (for 10-class classification)
'''
    </code></pre>
</div>
</section>

<section id="topic3">
<div class="summary">
    <h1>Set up the Loss Function and optimizer parameters</h1>
    <p><small>
       After building your model with `tf.keras.Sequential` and `Dense` layers at Stage II, you must <mark>compile</mark> it with:
       <ol>
        <li>A <mark>loss function</mark> — tells the model what to minimize</li>
        <li>An <mark>optimizer</mark> — tells the model *how* to update weights</li>
        <li>(Optional) <mark>Metrics</mark> — to monitor performance (e.g., accuracy)</li>
       </ol>
    </small></p>
    <h2>Bayes Theorem</h2>
        <p><small>
    <table border="1">
    <tr>
        <td>Type</td>
        <td>Output Shape</td>
        <td>Label Format</td>
        <td>loss Function</td>
    </tr>
    <tr>
        <td>Binary Classification</td>
        <td>`(None, 1)`</td>
        <td>0 or 1</td>
        <td>`binary_crossentropy;`</td>
    </tr>
    <tr>
        <td>Multi-Class Classification</td>
        <td>`(None, N)`</td>
        <td>Integer (0,1,2...)</td>
        <td>`sparse_categorical_crossentropy`</td>
    </tr>
    <tr>
        <td>Multi-Class (One-Hot)</td>
        <td>`(None, N)`</td>
        <td>One-hot vectors</td>
        <td>`categorical_crossentropy`</td>
    </tr>
    <tr>
        <td>Regression</td>
        <td>`(None, 1)`or more</td>
        <td>Continuous values</td>
        <td>`mse`(Mean Squared Error)</td>
    </tr>
</table>
        <mark>Tip</mark>: Use `sparse_categorical_crossentropy` if your labels are integers (e.g., `3`), 
        and `categorical_crossentropy` if they’re one-hot (e.g., `[0,0,0,1,0,0,0,0,0,0]`).
        </small></p>
    
</div>
</section>


<section>
<div class="discussion">
    <h2>Optimizers — How the Model Learns</h2>
    <p><small>
      <ol>
        <li>SGD — Stochastic Gradient Descent: Simple, classic optimizer, Often needs tuning (learning rate, momentum)</li>
        <li>Adam — Adaptive Moment Estimation: Usually works well out-of-the-box, Adaptive learning rate per parameter, Default learning rate = `0.001`</li>
      </ol>
      <ol>
        <li>Learning Rate — The Most Important Hyperparameter</li>
      </ol> 
      <pre><code>
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
      </code></pre>
    </small></p>
 </div>
</section>

<section>
<div class="discussion">
    <h2>Train a Model</h2>
    <p><small>
       In Stage IV, we need the `epochs`, `batch_size`, and `verbose` — are fundamental to training neural networks. 
       They control <mark>how long</mark>, <mark>how much data at a time</mark>, and <mark>how much feedback</mark> you get during training.
    <ol>
        <li>One <mark>epoch</mark> = one full pass over the entire training dataset. 
            If you have 1000 training samples and set `epochs=5` → model sees all 1000 samples <mark>5 times</mark>.</li>
        <li><mark>batch_size</mark>: How Many Samples to Process Before Updating Weights. 
            Training data is split into `small groups` called batches.
            For Rxample, batch_size=32 means that each batch has <mark>32 samples</mark>. 
            If we have 1,000 training samples → ~32 batches per epoch (`1000 / 32 ≈ 31.25`). 
            Common batch sizes: `16`, `32`, `64`, `128`</li>  
        <li>`verbose=1`: **Progress bar**(default) — shows % done, loss, metrics</li>      
    </ol>
    <pre><code>
# Train the model
model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1)
    </code></pre>
    
    </small></P>
    
</div>
</section>

<section >
<div class="summary">
    <h2>Evaluating the trained models</h2>
    <p><small>
       In Stage V Testing how well your trained model performs on **unseen data** (`X_test`, `y_test`).
       <pre><code>
##### Test data
X_test = np.random.rand(200, 100)  # 200 samples, 100 features
y_test = np.random.randint(0, 10, size=(200,))  # 10 classes
y_test = LabelBinarizer().fit_transform(y_test)  # One-hot encode labels

####### Evaluate on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
       </code></pre> 
    </small></p>
</div>
</section>

<section id="topic">
<div class="discussion">
    <h2>Plot for loss and accuracy</h2>
    <p><small>
        To understand how your model is learning — and whether it's overfitting, underfitting, or just right in Stage VI.
    </small></p>
    <pre><code>
import matplotlib.pyplot as plt
# Train the model and track metrics
history = model.fit(X_train, y_train, epochs=1000, batch_size=32, verbose=1,
                    validation_data=(X_test, y_test))

# Extract metrics from history
train_loss = history.history['loss']
train_accuracy = history.history['accuracy']
test_loss = history.history['val_loss']
test_accuracy = history.history['val_accuracy']
epochs = range(1, 11)

# Plot loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, test_loss, label='Test Loss')
plt.title('Training and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracy, label='Training Accuracy')
plt.plot(epochs, test_accuracy, label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
    </code></pre>
</div>
</section>

<section >
<div class="summary">
    <p><small>
      <ol>
        <li>Healthy Training (Good Fit)</li>
        <li>
            <pre><code>
Train Loss ──↘  
Val Loss ─────↘ (close to train)

Train Acc ──↗  
Val Acc ─────↗ (close to train)
            </code></pre>
        </li>
        <li>Overfitting</li>
        <li>
            <pre><code>
Train Loss ────────↘↘↘ (keeps dropping)
Val Loss ──────↘↗↗ (starts rising after epoch 5)

Train Acc ─────────↗↗↗
Val Acc ──────↗↘ (peaks then drops)
            </code></pre>
        </li>
        <li>Underfitting</li>
        <li>
            <pre><code>
Train Loss ──────── flat
Val Loss ─────────── flat
            </code></pre>
        </li>
      </ol> 
    </small></p>
</div>
</section>

<section>
<div class="discussion">
    <h2>Plot confusion matrix or classification report</h2>
    <p><small>
        Sometimes, After model.fit(), we can plot confusion matrix or classification report in Stage VII 
    </small></p>
    <pre><code>
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
# ----------------------------
# Predictions
# ----------------------------
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# ----------------------------
# Classification Report
# ----------------------------
print("\n" + "="*60)
print("CLASSIFICATION REPORT")
print("="*60)
print(classification_report(y_test, y_pred_classes))

# ----------------------------
# Confusion Matrix
# ----------------------------
cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=range(10), yticklabels=range(10))
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()
    </code></pre>
</div>
</section>

<section >
<div class="summary">
    <pre><code>
import numpy as np
import tensorflow as tf 
from sklearn.preprocessing import LabelBinarizer

# Stage I: Generate data
np.random.seed(42)  # For reproducibility
X_train = np.random.rand(1000, 100).astype(np.float32)  # 1000 samples, 100 features
y_train = np.random.randint(0, 10, size=(1000,))  # 10 classes
y_train = LabelBinarizer().fit_transform(y_train)  # One-hot encode labels

# Stage II: Define a simple sequential model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(100,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Stage III: Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Stage IV: Train the model with validation data
X_test = np.random.rand(200, 100).astype(np.float32)  # 200 samples, 100 features
y_test = np.random.randint(0, 10, size=(200,))  # 10 classes
y_test_one_hot = LabelBinarizer().fit_transform(y_test)  # One-hot encode labels for model evaluation

history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1,
                    validation_data=(X_test, y_test_one_hot))  # Use one-hot encoded y_test

# Stage V: Evaluate on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

# Extract metrics from history
train_loss = history.history['loss']
train_accuracy = history.history['accuracy']
test_loss = history.history['val_loss']
test_accuracy = history.history['val_accuracy']
epochs = range(1, len(train_loss) + 1)  # Corrected to match number of epochs

# Stage VI: Plot loss and accuracy
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, test_loss, label='Test Loss')
plt.title('Training and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_accuracy, label='Training Accuracy')
plt.plot(epochs, test_accuracy, label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Stage VII: Predictions
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = y_test  # Use original integer labels for classification report and confusion matrix

# Classification Report
print("\n" + "="*60)
print("CLASSIFICATION REPORT")
print("="*60)
print(classification_report(y_test_classes, y_pred_classes))

# Confusion Matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=range(10), yticklabels=range(10))
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()        
    </code></pre>
</div>
</section>



</body>
</html>