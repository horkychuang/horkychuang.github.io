{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d617dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"我愛打羽球，也喜歡看NBA比賽。\"\n",
    "\n",
    "# Cut into words\n",
    "words = jieba.lcut(text, cut_all=True)\n",
    "print(\"Segmented words:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Custom Words\n",
    "jieba.add_word('大模型')\n",
    "jieba.add_word('AIGC')\n",
    "\n",
    "text = \"大模型和AIGC是當前王道\"\n",
    "print(jieba.lcut(text))\n",
    "\n",
    "# Load Custom Dictionary\n",
    "# jieba.load_userdict(\"mydict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "stop_words = {\n",
    "    '的', '了', '是', '在', '我', '有', '和', '就', '不', '都', '一', '一個', '上', '也', '很',\n",
    "    '到', '說', '要', '去', '你', '會', '著', '看', '這', '那', '他', '她', '它', '們', '為',\n",
    "    '能', '個', '可以', '什麼', '怎麼', '這麼', '哪裡', '時候', '覺得', '因為', '所以',\n",
    "    '但是', '如果', '就', '還是', '還是', '還是', '嗎', '吧', '呢', '啊', '呀'\n",
    "}\n",
    "\n",
    "\n",
    "def clean_chinese_text(text, stop_words=stop_words):\n",
    "    # Step 1: Remove English letters (a-z, A-Z) and words\n",
    "    text = re.sub(r'[a-zA-Z]+', '', text)\n",
    "    \n",
    "    # Step 2: Remove numbers (0-99, 100, etc.)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 3: Remove punctuation (common Chinese & English punctuation)\n",
    "    # You can customize this list\n",
    "    punctuation = r\"[，。！？；：\"\"''（）【】《》、·…\\-\\s]+\"\n",
    "    text = re.sub(punctuation, ' ', text)\n",
    "    \n",
    "    # Step 4: Use jieba to segment text into words\n",
    "    words = jieba.lcut(text)\n",
    "    \n",
    "    # Step 5: Filter words\n",
    "    cleaned_words = [\n",
    "        word for word in words\n",
    "        if word not in stop_words      # Remove stop words\n",
    "        and len(word) > 1              # Remove single characters (optional)\n",
    "        and not re.match(r'^\\s*$', word)  # Remove whitespace\n",
    "    ]\n",
    "    \n",
    "    # Return as list of words, or join into clean text\n",
    "    return cleaned_words  # or ' '.join(cleaned_words)\n",
    "\n",
    "raw_text = \"\"\"\n",
    "我昨天買了3個蘋果和2瓶可樂，花了199元！\n",
    "This is a test message with English words like AI and deep learning.\n",
    "我覺得這個價格還可以，但是下次還是去超市買比較好。\n",
    "#優惠 #促銷\n",
    "\"\"\"\n",
    "\n",
    "# Keep only Chinese characters (Unicode range)\n",
    "words = jieba.lcut(text)\n",
    "cleaned = [w for w in words if w.strip() and w not in stop_words]\n",
    "cleaned = clean_chinese_text(raw_text)\n",
    "print(\"Cleaned words:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b84558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Simulate more text (like from news or user input)\n",
    "long_text = \"\"\"\n",
    "我愛打籃球，也喜歡看NBA比賽。籃球比賽非常精彩，球員技術高超。\n",
    "Taiwan男籃正在備戰亞洲杯，希望取得好成績。\n",
    "\"\"\"\n",
    "\n",
    "words = jieba.lcut(long_text)\n",
    "filtered_words = [w for w in words if w not in stop_words and len(w) > 1]\n",
    "\n",
    "# Count frequency\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Top words:\", word_freq.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bebbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# List all fonts and filter those likely supporting Chinese\n",
    "chinese_fonts = [f.name for f in fm.fontManager.ttflist if any(kw in f.name.lower() for kw in ['hei', 'song', 'kai', 'fang', 'noto', 'ming', 'msyh'])]\n",
    "print(\"Available Chinese-capable fonts:\")\n",
    "for font in sorted(set(chinese_fonts)):\n",
    "    print(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# --- Auto-detect Chinese font ---\n",
    "def get_chinese_font():\n",
    "    possible_fonts = [\n",
    "        'Microsoft YaHei',\n",
    "        'SimHei',\n",
    "        'PingFang TC',\n",
    "        'PingFang SC',\n",
    "        'Noto Sans CJK TC',\n",
    "        'Noto Sans CJK SC',\n",
    "        'FangSong',\n",
    "        'KaiTi'\n",
    "    ]\n",
    "    available_fonts = set(f.name for f in fm.fontManager.ttflist)\n",
    "    for font in possible_fonts:\n",
    "        if font in available_fonts:\n",
    "            return font\n",
    "    # Fallback: use first available font containing 'Hei' or 'Song'\n",
    "    for f in fm.fontManager.ttflist:\n",
    "        if any(kw in f.name for kw in ['Hei', 'Song', 'YaHei', 'PingFang', 'Noto']):\n",
    "            return f.name\n",
    "    return None  # If none found, may still fail\n",
    "\n",
    "# Set font\n",
    "chinese_font = get_chinese_font()\n",
    "if chinese_font:\n",
    "    plt.rcParams['font.sans-serif'] = [chinese_font]\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    print(f\" Using font: {chinese_font}\")\n",
    "else:\n",
    "    print(\"No suitable Chinese font found. May not display correctly.\")\n",
    "\n",
    "# --- Your existing code ---\n",
    "stop_words = {\n",
    "    '的', '了', '是', '在', '我', '有', '和', '就', '不', '都', '一', '一個', '上', '也', '很',\n",
    "    '到', '說', '要', '去', '你', '會', '著', '看', '這', '那', '他', '她', '它', '們', '為',\n",
    "    '能', '個', '可以', '什麼', '怎麼', '這麼', '哪裡', '時候', '覺得', '因為', '所以',\n",
    "    '但是', '如果', '嗎', '吧', '呢', '啊', '呀', '了', '啦', '哦'\n",
    "}\n",
    "\n",
    "def clean_and_segment(text):\n",
    "    text = re.sub(r'[a-zA-Z0-9]+', '', text)  # Remove digits and English\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', text)  # Keep only Chinese chars\n",
    "    words = jieba.lcut(text)\n",
    "    filtered_words = [\n",
    "        word for word in words\n",
    "        if word not in stop_words\n",
    "        and len(word) > 1\n",
    "        and word.strip()\n",
    "    ]\n",
    "    return filtered_words\n",
    "\n",
    "text = \"\"\"\n",
    "我愛打籃球，籃球比賽非常精彩！NBA是世界上最好的籃球聯賽。\n",
    "我喜歡看湖人隊和勇士隊的比賽，球員技術高超，配合默契。\n",
    "Taiwan男籃正在努力提升水準。\n",
    "\"\"\"\n",
    "\n",
    "words = clean_and_segment(text)\n",
    "print(\"Segmented & cleaned words:\", words)\n",
    "\n",
    "word_freq = Counter(words)\n",
    "print(\"\\nTop 10 most frequent words:\")\n",
    "for word, freq in word_freq.most_common(10):\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Plot bar chart\n",
    "top_words = word_freq.most_common(10)\n",
    "words_only = [item[0] for item in top_words]\n",
    "counts = [item[1] for item in top_words]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words_only, counts, color='skyblue')\n",
    "plt.title('Top 10 Chinese Words by Frequency')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Prevent label cutoff\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Sample Chinese text\n",
    "text = \"\"\"\n",
    "自然語言處理是人工智慧的重要分支，它讓電腦能夠理解、分析和生成人類語言。\n",
    "近年來，隨著深度學習的發展，自然語言處理在機器翻譯、情感分析、問答系統等領域取得了顯著進展。\n",
    "中文文本處理尤其具有挑戰性，因為中文沒有明顯的詞邊界，需要先進行分詞。\n",
    "自然語言處理的應用非常廣泛，包括智慧客服、搜尋引擎、語音助手等。\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Segment text using jieba\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# Step 2: Optional — Filter out stopwords (you can load a custom list)\n",
    "# Here’s a minimal stopword list for demo\n",
    "stopwords = {'的', '是', '在', '和', '等', '、', '，', '。', '隨著', '因為', '需要', '先', '它', '讓', '了', '非常'}\n",
    "\n",
    "# Filter out stopwords and single-character tokens (optional, to reduce noise)\n",
    "filtered_words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "\n",
    "# Step 3: Count term frequencies\n",
    "term_freq = Counter(filtered_words)\n",
    "\n",
    "# Step 4: Display top 10 most frequent terms\n",
    "print(\"=== Top 10 Term Frequencies ===\")\n",
    "top_terms = term_freq.most_common(10)\n",
    "\n",
    "for term, freq in top_terms:\n",
    "    print(f\"{term}: {freq}\")\n",
    "\n",
    "# Optional: Convert to DataFrame for better visualization\n",
    "df = pd.DataFrame(top_terms, columns=['Term', 'Frequency'])\n",
    "print(\"\\n=== As DataFrame ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f90b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df['Term'][::-1], df['Frequency'][::-1], color='steelblue')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 10 Terms by Frequency in Chinese Text')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fe32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Simulate 3 short \"documents\" (e.g., news articles)\n",
    "docs = [\n",
    "    \"我愛籃球 NBA比賽精彩 球員厲害\",\n",
    "    \"足球是世界第一運動 梅西是天才\",\n",
    "    \"Taiwan籃球運動員努力\"\n",
    "]\n",
    "\n",
    "# Segment each document\n",
    "segmented_docs = []\n",
    "for doc in docs:\n",
    "    words = jieba.lcut(doc)\n",
    "    cleaned = \" \".join([w for w in words if w not in stop_words])\n",
    "    segmented_docs.append(cleaned)\n",
    "\n",
    "print(\"Processed docs:\", segmented_docs)\n",
    "\n",
    "# apply TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(segmented_docs)\n",
    "\n",
    "# Show feature names and TF-IDF values\n",
    "print(\"Features:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF shape:\", X.shape)  # (3 docs, N features)\n",
    "print(\"TF-IDF matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "docs = [\n",
    "    \"我愛打籃球，NBA比賽非常精彩，湖人隊和勇士隊對決。\",\n",
    "    \"Taiwan男籃正在努力提升水準。\",\n",
    "    \"人工智慧是未來的方向，AIGC技術正在快速發展。\",\n",
    "    \"我喜歡看足球比賽，世界盃每四年舉辦一次。\"\n",
    "]\n",
    "\n",
    "stop_words = {\n",
    "    '的', '了', '是', '在', '我', '有', '和', '就', '不', '都', '一', '一個', '上', '也', '很',\n",
    "    '到', '說', '要', '去', '你', '會', '著', '看', '這', '那', '他', '她', '它', '們', '為',\n",
    "    '能', '個', '可以', '什麼', '怎麼', '這麼', '哪裡', '時候', '覺得', '因為', '所以',\n",
    "    '但是', '如果', '嗎', '吧', '呢', '啊', '呀'\n",
    "}\n",
    "\n",
    "def clean_and_segment(doc):\n",
    "    import re\n",
    "    # Remove English letters and numbers\n",
    "    doc = re.sub(r'[a-zA-Z0-9]+', '', doc)\n",
    "    # Keep only Chinese characters\n",
    "    doc = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', doc)\n",
    "    \n",
    "    # Jieba segmentation\n",
    "    words = jieba.lcut(doc)\n",
    "    \n",
    "    # Filter\n",
    "    filtered = [\n",
    "        word for word in words\n",
    "        if word not in stop_words\n",
    "        and len(word) > 1\n",
    "    ]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# Process all documents\n",
    "processed_docs = [clean_and_segment(doc) for doc in docs]\n",
    "print(\"Processed docs:\")\n",
    "for i, d in enumerate(processed_docs):\n",
    "    print(f\"Doc {i+1}: {d}\")\n",
    "    \n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), index=[f'Doc {i+1}' for i in range(len(docs))], columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(df_tfidf.round(3))    \n",
    "\n",
    "def get_top_keywords(tfidf_matrix, feature_names, doc_index, top_n=3):\n",
    "    # Get TF-IDF scores for the document\n",
    "    doc_scores = tfidf_matrix[doc_index].toarray()[0]\n",
    "    # Sort by score\n",
    "    word_score_pairs = list(zip(feature_names, doc_scores))\n",
    "    word_score_pairs = sorted(word_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    return word_score_pairs[:top_n]\n",
    "\n",
    "# Example: Top words in Doc 1\n",
    "top_words = get_top_keywords(tfidf_matrix, feature_names, doc_index=0, top_n=5)\n",
    "print(\"\\nTop words in Doc 1:\")\n",
    "for word, score in top_words:\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "# Sample Chinese text\n",
    "text = \"\"\"\n",
    "自然語言處理是人工智慧的重要分支，它讓電腦能夠理解、分析和生成人類語言。\n",
    "近年來，隨著深度學習的發展，自然語言處理在機器翻譯、情感分析、問答系統等領域取得了顯著進展。\n",
    "中文文本處理尤其具有挑戰性，因為中文沒有明顯的詞邊界，需要先進行分詞。\n",
    "\"\"\"\n",
    "\n",
    "# TF-IDF based keyword extraction\n",
    "print(\"=== TF-IDF Keywords ===\")\n",
    "keywords_tfidf = jieba.analyse.extract_tags(text, topK=5, withWeight=True, allowPOS=())\n",
    "\n",
    "for keyword, weight in keywords_tfidf:\n",
    "    print(f\"{keyword}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "x=df_tfidf\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "print(\"Similarity matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your sample documents\n",
    "docs = [\n",
    "    \"我愛打籃球，NBA比賽非常精彩，湖人隊和勇士隊對決。\",\n",
    "    \"Taiwan男籃正在努力提升水準。\",\n",
    "    \"人工智慧是未來的方向，AIGC技術正在快速發展。\",\n",
    "    \"我喜歡看足球比賽，世界盃每四年舉辦一次。\"\n",
    "]\n",
    "\n",
    "# Combine all documents into one string\n",
    "text = \" \".join(docs)\n",
    "\n",
    "# Use jieba to segment Chinese text\n",
    "seg_list = jieba.lcut(text)\n",
    "seg_text = \" \".join(seg_list)\n",
    "\n",
    "# Generate word cloud\n",
    "# Use a font that supports Chinese characters (e.g., Noto Sans CJK, SimHei, etc.)\n",
    "# You may need to specify the path to a Chinese font on your system.\n",
    "# For example, on Windows: 'msyh.ttc' (Microsoft YaHei)\n",
    "# On macOS: '/System/Library/Fonts/PingFang.ttc'\n",
    "# Or download and use 'NotoSansCJK-Regular.ttc'\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    font_path='msyh.ttc',  # Change this to a valid Chinese font path on your system\n",
    "    width=800,\n",
    "    height=600,\n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    colormap='viridis'\n",
    ").generate(seg_text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Chinese Word Cloud from Sample Documents\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "\n",
    "# ========================\n",
    "# 1. 準備中文文本資料\n",
    "# ========================\n",
    "texts = [\n",
    "    \"美國經濟持續增長，金融市場穩定發展，銀行利率下調促進消費\",\n",
    "    \"美國總統大選臨近，候選人紛紛發表演講爭取選民支持\",\n",
    "    \"人工智慧技術快速發展，深度學習在圖像識別領域取得突破\",\n",
    "    \"央行發佈新政策，鼓勵中小企業貸款，穩定就業市場\",\n",
    "    \"科技公司推出新一代智慧手機，搭載AI晶片提升性能\",\n",
    "    \"選舉投票率創新高，民眾積極參與民主進程\",\n",
    "    \"金融科技結合大資料，提升風險控制能力\",\n",
    "    \"神經網路模型在自然語言處理中表現優異\",\n",
    "    \"政府推動綠色能源，減少碳排放保護環境\",\n",
    "    \"深度學習框架如PyTorch和TensorFlow廣受歡迎\"\n",
    "]\n",
    "\n",
    "# ========================\n",
    "# 2. 中文預處理：分詞 + 去停用詞\n",
    "# ========================\n",
    "# 簡易中文停用詞表（實際專案建議使用完整停用詞表）\n",
    "stop_words = set([\n",
    "    '的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '一個', '上', '也', '很', '到',\n",
    "    '說', '要', '去', '你', '會', '著', '沒有', '看', '好', '自己', '這', '那', '個', '之', '與', '及',\n",
    "    '為', '對', '於', '並', '等', '後', '而', '以', '得', '地', '她', '他', '它', '們', '能', '又', '可',\n",
    "    '從', '但', '還', '或', '即', '其', '已', '些', '下', '被', '給', '讓', '由', '向', '往', '把', '將'\n",
    "])\n",
    "\n",
    "def preprocess_chinese(texts):\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        words = jieba.lcut(text)\n",
    "        filtered_words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "        tokenized_texts.append(filtered_words)\n",
    "    return tokenized_texts\n",
    "\n",
    "print(\" 正在預處理中文文本...\")\n",
    "processed_texts = preprocess_chinese(texts)\n",
    "for i, doc in enumerate(processed_texts):\n",
    "    print(f\" 文檔{i+1}: {doc}\")\n",
    "\n",
    "# ========================\n",
    "# 3. 構建詞典 + 語料 + 訓練LDA模型\n",
    "# ========================\n",
    "print(\"\\n 正在構建詞典和語料...\")\n",
    "dictionary = Dictionary(processed_texts)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)  # 過濾極端詞\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "print(\"正在訓練LDA主題模型（3個主題）...\")\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "print(\" LDA 主題模型訓練完成！\")\n",
    "\n",
    "# ========================\n",
    "# 4. 主題表示 1：詞 + 概率分佈\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" 主題表示 1：每個主題的關鍵字及其概率\")\n",
    "print(\"=\"*50)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\" 主題 {idx}: {topic}\")\n",
    "\n",
    "# ========================\n",
    "# 5. 主題表示 2：Top-N 關鍵字清單\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" 主題表示 2：每個主題的 Top 5 關鍵字（簡潔版）\")\n",
    "print(\"=\"*50)\n",
    "for i in range(lda_model.num_topics):\n",
    "    topic_words = [word for word, prob in lda_model.show_topic(i, topn=5)]\n",
    "    print(f\" 主題 {i}: {', '.join(topic_words)}\")\n",
    "\n",
    "# ========================\n",
    "# 6. 主題表示 3：詞雲視覺化（每個主題）\n",
    "# ========================\n",
    "print(\"\\n 正在生成詞雲圖...（請稍等）\")\n",
    "\n",
    "def plot_wordcloud_for_topic(topic_id, lda_model, dictionary, figsize=(10, 5)):\n",
    "    word_dict = {}\n",
    "    for word_id, prob in lda_model.get_topic_terms(topic_id, topn=20):\n",
    "        word = dictionary[word_id]\n",
    "        word_dict[word] = prob\n",
    "\n",
    "    # 生成詞雲\n",
    "    wordcloud = WordCloud(\n",
    "            font_path='msyh.ttc',  # Windows 系統自帶\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(word_dict)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic {topic_id} wordcloud\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 為每個主題繪製詞雲\n",
    "for i in range(lda_model.num_topics):\n",
    "    plot_wordcloud_for_topic(i, lda_model, dictionary)\n",
    "\n",
    "# ========================\n",
    "# 7. 主題表示 4：文檔-主題分佈表\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" 主題表示 4：每篇文檔的主題分佈（概率表）\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "doc_topics_df = pd.DataFrame()\n",
    "for i, doc_bow in enumerate(corpus):\n",
    "    topic_probs = lda_model.get_document_topics(doc_bow, minimum_probability=0.01)\n",
    "    probs_dict = {f\"主題_{topic}\": prob for topic, prob in topic_probs}\n",
    "    doc_topics_df = pd.concat([doc_topics_df, pd.DataFrame([probs_dict])], ignore_index=True)\n",
    "\n",
    "# 補全缺失列 + 設置索引\n",
    "for col in [f\"主題_{i}\" for i in range(lda_model.num_topics)]:\n",
    "    if col not in doc_topics_df.columns:\n",
    "        doc_topics_df[col] = 0.0\n",
    "\n",
    "doc_topics_df = doc_topics_df[[f\"主題_{i}\" for i in range(lda_model.num_topics)]].fillna(0)\n",
    "doc_topics_df.index = [f\"文檔{i+1}\" for i in range(len(corpus))]\n",
    "doc_topics_df = doc_topics_df.round(3)\n",
    "\n",
    "print(doc_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc525da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Positive words (partial list)\n",
    "positive_words = {\n",
    "    '好', '棒', '贊', '喜歡', '愛', '優秀', '精彩', '滿意', '推薦', '強',\n",
    "    '太棒了', '非常好', '很不錯', '喜歡', '熱愛', '開心', '幸福', '成功'\n",
    "}\n",
    "\n",
    "# Negative words\n",
    "negative_words = {\n",
    "    '差', '爛', '討厭', '噁心', '糟糕', '失望', '差勁', '垃圾', '坑人', '難用',\n",
    "    '不滿意', '很差', '不喜歡', '難過', '傷心', '失敗'\n",
    "}\n",
    "\n",
    "# Clean & Segment Text\n",
    "import jieba\n",
    "\n",
    "def clean_and_segment(text):\n",
    "    import re\n",
    "    # Keep only Chinese characters\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text)\n",
    "    words = jieba.lcut(text)\n",
    "    return [w for w in words if len(w) > 0]\n",
    "\n",
    "# Test\n",
    "text = \"這部電影太棒了，我非常喜歡！\"\n",
    "words = clean_and_segment(text)\n",
    "print(\"Words:\", words)\n",
    "\n",
    "def get_sentiment_score(text, pos_words=positive_words, neg_words=negative_words):\n",
    "    words = clean_and_segment(text)\n",
    "    \n",
    "    pos_count = sum(1 for w in words if w in pos_words)\n",
    "    neg_count = sum(1 for w in words if w in neg_words)\n",
    "    \n",
    "    # Simple scoring\n",
    "    if pos_count > neg_count:\n",
    "        return \"Positive\", pos_count - neg_count\n",
    "    elif neg_count > pos_count:\n",
    "        return \"Negative\", neg_count - pos_count\n",
    "    else:\n",
    "        return \"Neutral\", 0\n",
    "\n",
    "# Test\n",
    "texts = [\n",
    "    \"這部電影太棒了，我非常喜歡！\",\n",
    "    \"服務很差，餐廳環境也很差。\",\n",
    "    \"今天天氣是陰天。\",\n",
    "    \"產品不錯，但物流太慢了。\"\n",
    "]\n",
    "\n",
    "print(\"Lexicon-Based Sentiment Analysis:\")\n",
    "for t in texts:\n",
    "    sent, score = get_sentiment_score(t)\n",
    "    print(f\"'{t}' → {sent} (score: {score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install snownlp\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "def snownlp_sentiment(text):\n",
    "    s = SnowNLP(text)\n",
    "    prob = s.sentiments  # 0 = negative, 1 = positive\n",
    "    return \"Positive\" if prob > 0.6 else \"Negative\" if prob < 0.4 else \"Neutral\", round(prob, 3)\n",
    "\n",
    "print(\"\\nSnowNLP Sentiment:\")\n",
    "texts = [\n",
    "    \"這部電影太棒了，我非常喜歡！\",\n",
    "    \"服務很差，餐廳環境也很差。\",\n",
    "    \"今天天氣是陰天。\",\n",
    "    \"產品不錯，但物流太慢了。\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    sent, prob = snownlp_sentiment(t)\n",
    "    print(f\"'{t}' → {sent} (score: {prob})\")\n",
    "    \n",
    "reviews = [\n",
    "    \"手機很好用，拍照清晰，運行流暢。\",\n",
    "    \"電池續航太差了，一天要充三次電。\",\n",
    "    \"外觀漂亮，但系統卡頓嚴重。\",\n",
    "    \"性價比很高，推薦購買！\",\n",
    "    \"客服態度差，售後不解決問題。\"\n",
    "]\n",
    "print(\"\\nProduct Review Sentiment Analysis:\")\n",
    "for r in reviews:\n",
    "    sentiment, probs = snownlp_sentiment(r)  # ← probs is a FLOAT\n",
    "    print(f\" {r}\")\n",
    "    print(f\"   → {sentiment} (score: {probs})\\n\")  # ← Just print the float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "\n",
    "# ==============================\n",
    "# 1. 準備中文標註數據集 (模擬電商評論)\n",
    "# ==============================\n",
    "data = [\n",
    "    (\"這款手機太棒了，拍照非常清晰，電池也很耐用\", \"positive\"),\n",
    "    (\"客服態度極差，等了半天也沒人回覆\", \"negative\"),\n",
    "    (\"物流很快，包裝完好，非常滿意\", \"positive\"),\n",
    "    (\"質量很差，用了一個星期就壞了\", \"negative\"),\n",
    "    (\"性價比超高，功能齊全，推薦購買\", \"positive\"),\n",
    "    (\"螢幕有壞點，退貨流程麻煩\", \"negative\"),\n",
    "    (\"系統流暢，外觀漂亮，物超所值\", \"positive\"),\n",
    "    (\"充電器是假貨，商家不承認\", \"negative\"),\n",
    "    (\"音質出色，續航給力，值得入手\", \"positive\"),\n",
    "    (\"客服不理人，售後推卸責任\", \"negative\"),\n",
    "    (\"運行速度快，遊戲無壓力\", \"positive\"),\n",
    "    (\"收到貨發現是二手翻新機\", \"negative\"),\n",
    "    (\"贈品豐富，服務周到\", \"positive\"),\n",
    "    (\"宣傳與實物嚴重不符\", \"negative\"),\n",
    "    (\"手感好，顏值高，朋友都說好看\", \"positive\")\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "print(\" 原始數據樣本：\")\n",
    "print(df.head(10))\n",
    "\n",
    "# ==============================\n",
    "# 2. 中文文本預處理函數\n",
    "# ==============================\n",
    "def preprocess_chinese_text(text):\n",
    "    # 去除標點、數字、特殊符號（保留中文、英文字母）\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z]', ' ', text)\n",
    "    # 使用 jieba 分詞\n",
    "    words = jieba.lcut(text)\n",
    "    # 過濾停用詞和單字\n",
    "    stop_words = set(['的', '了', '在', '是', '我', '有', '和', '就', '不', '人', '都', '一', '個', '上', '也', '很', '到', '說', '要', '去'])\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(words)  # 返回空格分隔的字串，供TF-IDF使用\n",
    "\n",
    "# 應用預處理\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_chinese_text)\n",
    "\n",
    "print(\"\\n 預處理後的文本示例：\")\n",
    "for i in range(3):\n",
    "    print(f\"原文: {df.iloc[i]['text']}\")\n",
    "    print(f\"處理後: {df.iloc[i]['cleaned_text']}\\n\")\n",
    "\n",
    "# ==============================\n",
    "# 3. 劃分訓練集和測試集\n",
    "# ==============================\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" 訓練集大小: {len(X_train)}, 測試集大小: {len(X_test)}\")\n",
    "\n",
    "# ==============================\n",
    "# 4. 手動 TF-IDF 向量化（不使用 Pipeline）\n",
    "# ==============================\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    token_pattern=r'(?u)\\b\\w+\\b'\n",
    ")\n",
    "\n",
    "#  重要：只在訓練集上 fit\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "#  測試集只 transform，不 fit！\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\" 訓練集向量形狀: {X_train_tfidf.shape}\")\n",
    "print(f\" 測試集向量形狀: {X_test_tfidf.shape}\")\n",
    "\n",
    "# ==============================\n",
    "# 5. 訓練分類器（手動，不用 Pipeline）\n",
    "# ==============================\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\" 模型訓練完成！\")\n",
    "\n",
    "# ==============================\n",
    "# 6. 評估模型\n",
    "# ==============================\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n 分類報告:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))\n",
    "\n",
    "print(\"\\n 混淆矩陣:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ==============================\n",
    "# 7. 預測新評論\n",
    "# ==============================\n",
    "new_comments = [\n",
    "    \"手機非常好用，強烈推薦！\",\n",
    "    \"垃圾產品，客服也不管\",\n",
    "    \"還行吧，沒什麼特別的\",\n",
    "    \"物流慢，但產品質量不錯\"\n",
    "]\n",
    "\n",
    "print(\"\\n 新評論情感預測:\")\n",
    "for comment in new_comments:\n",
    "    # 預處理\n",
    "    cleaned = preprocess_chinese_text(comment)\n",
    "    # 向量化（使用訓練好的 vectorizer）\n",
    "    comment_vec = vectorizer.transform([cleaned])\n",
    "    # 預測\n",
    "    pred = classifier.predict(comment_vec)[0]\n",
    "    prob = classifier.predict_proba(comment_vec)[0]\n",
    "    confidence = max(prob)\n",
    "    print(f\"原文: {comment}\")\n",
    "    print(f\"預測: {pred} (置信度: {confidence:.2f})\\n\")\n",
    "\n",
    "# ==============================\n",
    "# 8. 查看特徵重要性（哪些詞最影響分類）\n",
    "# ==============================\n",
    "def show_top_features(vectorizer, classifier, class_names=['negative', 'positive'], top_n=10):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = classifier.coef_[0]  # binary classification\n",
    "\n",
    "    top_negative = sorted(zip(coef, feature_names))[:top_n]\n",
    "    top_positive = sorted(zip(coef, feature_names), reverse=True)[:top_n]\n",
    "\n",
    "    print(f\"\\n 最具負面情感的詞 (Top {top_n}):\")\n",
    "    for coef_val, word in top_negative:\n",
    "        print(f\"  {word}: {coef_val:.3f}\")\n",
    "\n",
    "    print(f\"\\n 最具正面情感的詞 (Top {top_n}):\")\n",
    "    for coef_val, word in top_positive:\n",
    "        print(f\"  {word}: {coef_val:.3f}\")\n",
    "\n",
    "show_top_features(vectorizer, classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
