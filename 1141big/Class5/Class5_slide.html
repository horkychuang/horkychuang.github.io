<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 5 Slides</title>
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <!-- Load MathJax -->
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <!-- Load Marked.js for Markdown parsing -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* General Slide Styling */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #000000; /* Changed to black as per body style */
        }
        .slide {
            display: none;
            width: 80%;
            max-width: 900px;
            min-height: 80vh;
            margin: 50px auto;
            padding: 20px;
            background: #FFF8DC; /* Light Yellow Background */
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            text-align: left;
            overflow-y: auto; /* Enable vertical scrolling if content overflows */
        }
        .slide.active {
            display: flex; /* Use flex for proper centering */
            flex-direction: column;
        }
        h1, h2, h3 {
            color: #34495e;
        }
        p, li {
            font-size: 18px;
            line-height: 1.6;
            color: #555;
        }
        pre {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 14px;
        }
        code {
            color: #e74c3c;
        }
        .controls {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 20px;
        }
        .controls button {
            padding: 10px 20px;
            font-size: 16px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            background-color: #3498db;
            color: white;
            transition: background-color 0.3s ease;
        }
        .controls button:hover {
            background-color: #2980b9;
        }
        .aa {
            background-color: #ecf0f1;
            padding: 15px;
            border-left: 5px solid #3498db;
            text-align: left;
            width: 100%;
        }
        .bb {
            background-color: #fef9e7;
            padding: 15px;
            border-left: 5px solid #f1c40f;
            text-align: left;
            width: 100%;
        }
    </style>
</head>

<body>
    <!-- Cover Slide: Class 3 Introduction -->
    <div class="slide active">
        <div style="height: 100%; display: flex; flex-direction: column; justify-content: space-between;">
            <!-- Image at the top -->
            <div style="text-align: center; padding-top: 20px;">
                <img src="images/04103.jpg" alt="04103" style="max-width: 100%; height: auto; max-height: 350px; border-radius: 8px;">
            </div>
            <div>
                <h1 style="text-align: center;">
                    Class 5 Ensemble Learning
                </h1>
                <h3 style="text-align: center; margin-top: 10px;">
                    Wen-Bin Chuang<br>
                    September 02, 2025<br>
                    NCNU, IBS
                </h3>
            </div>
        </div>
    </div>

    <!-- Slide 1 -->
    <div class="slide">
        <div class="aa">
            <h1>Introduction</h1>
    <p><small>
      “United we stand” is the motto for ensemble methods. 
      We have discussed and experimented with several supervised learning methods and learned how to evaluate them 
      and tune their performance. 
      Ensemble learning is a suite of techniques that uses multiple machine learning models 
      in order to obtain better performance than we could have from any of the models.<br>
      Ensemble learning allows us to collate the power of multiple models and then make a prediction. 
      These models individually are weak but together act as a strong model for prediction
    </small></p>
    <img src="images\2025063001.jpg" alt="2025063001" width="300">
    <p><small>
       Ensemble methods can be divided into two broad categories: bagging and boosting
       <ul>
        <li>Bagging uses sampling with replacement to generate multiple datasets. 
            It builds multiple predictors simultaneously and independently of each other</li>
        <li>In boosting, the learners are grown sequentially from the last one. 
            Each subsequent learner improves from the last iteration and focuses more on the errors in the last iteration</li>
       </ul> 
    </small></p>
        </div>
    </div>

    <!-- Slide 2 -->
    <div class="slide">
        <div class="bb">
            <h1>Bagging (Bootstrap Aggregating) Algorithms</h1>
    <p><small>
       <mark>Bagging</mark> is an ensemble machine learning technique designed to improve the stability and accuracy of algorithms 
       by combining multiple models. It works by: 
       <ul>
        <li>Creating multiple bootstrap samples (random samples with replacement) from the original datase</li>
        <li>Training a base model on each bootstrap sample</li>
        <li>Aggregating predictions through voting (classification) or averaging (regression)</li>
       </ul>
    </small></p>
    <h2>Random Forest</h2>
    <p><small>
       Random forest algorithm is an `ensemble method` and creates decision trees by using <mark>data resampling</mark> 
       and then gets the prediction from each of them and finally selects the best solution by means of voting or averaging.<br>
       A forest is made up of trees and more trees means more robust forest. 
       Random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting or averaging.
    </small></p>
        </div>
    </div>

    <!-- Slide 3 -->
    <div class="slide">
        <div class="aa">
            <p><small>
       Decision trees are constructed by selecting one of the attributes that provides the best separation of the classes. 
       This is done either by computing the information gain or Gini index.  
    </small></p>
    <img src="images\2025063002.jpg" alt="2025063002" width="500">
    <p><small>
       In random forest, The sample is generated randomly with each training data point having the equal probability of being sampled. 
       If N represents the size of training dataset and M is the number of random data points to be considered for training, 
       each point is randomly sampled with replacement.<br></small></p>
    </div></div>

    <div class="slide">
        <div class="aa">
    <p><small>   
       In the training process, we create k models, where k is a predefined number and a hyperparameter that is easy to configure. 
       Each tree is constructed independently, and thus, the process can be implemented in parallel. 
       Eventually, we will have k-independent decision trees with possibly different structures, 
       and they might give different results for the same test item. <br>
       if k=4 and the class labels predicted by the four models are 1, 0, 1, and 1, 
       there are three votes for 1 and one vote for 0. Thus, the final class that is assigned is 1
    </small></p>
    <img src="images\2025063003.jpg" alt="2025063003" width="600">
        </div>
    </div>

    <!-- Slide 4 -->
    <div class="slide">
        <div class="bb">
            <h1>Boosting Algorithms</h1>
    <p><small>
    In boosting, the ensemble is created <mark>incrementally</mark> by training a new model 
    from a subset of the training data that considers a more proportion of data points that the previous models misclassified.
    </small></p>
    <h2>AdaBoost Algorithms</h2>
    <p><small>
       Initially, AdaBoost assigns each training tuple an equal weight of 1/d. 
       Generating k classifiers for the ensemble requires k rounds through the rest of the algorithm. 
       In round i, the tuples from D are sampled to form a training set, Di, of size d. 
       Sampling with replacement is used – the same tuple may be selected more than once. 
       Each tuple’s chance of being selected is based on its weight. 
    </small></p>
    <img src="images\2025063005.jpg" alt="2025063005" width="400">
    <p><small>
       The weights of the training tuples are then adjusted according to how they were classified. 
       If a tuple was incorrectly classified, its weight is increased. 
       If a tuple was correctly classified, its weight is decreased. 
       A tuple’s weight reflects how difficult it is to classify – the higher the weight, the more often it has been misclassified.
    </small></p> 
        </div>
    </div>

    <!-- Slide 5 -->
    <div class="slide">
        <div class="aa">
            <h2>Another types of boosting algorithms</h2>
    <p><small>
       <ul>
        <li>Gradient boosting: The overall learner gradually improves on the observations where the residuals have been initially high.</li>
        <li>Extreme gradient boosting: Extreme gradient boosting of XGB is an advanced boosting algorithm. 
            It has become quite popular lately and has won many data science and ML competitions</li>
        <li>LightGBM (Light Gradient Boosting Machine) is a high-performance, distributed, and efficient gradient boosting framework developed by Microsoft. 
            It's designed for speed, low memory usage, and scalability on large datasets</li>    
       </ul> 
    </small></p>
        </div>
    </div>

    <!-- Slide 6 -->
    <div class="slide">
        <div class="bb">
            <h1>Stacking Algorithms</h1>
    <p><small>
        Stacking, or stacked generalization, is another ensemble learning technique 
        that combines the predictions from <mark>multiple machine learning models</mark> 
        by assigning the weights on individual constituent classifiers. Stacking (Stacked Generalization) is an advanced ensemble learning technique 
        that combines multiple base models (called `level-0 models` or `base learners`) 
        using a <mark>meta-model (level-1 model)</mark> to learn how to best combine their predictions.
    </small></p>
        </div>
    </div>

    

    <!-- Navigation Controls -->
    <div class="controls">
        <button onclick="prevSlide()">Previous</button>
        <button onclick="nextSlide()">Next</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');

        function showSlide(index) {
            slides.forEach((slide, i) => {
                slide.classList.toggle('active', i === index);
            });
        }

        function nextSlide() {
            currentSlide = (currentSlide + 1) % slides.length;
            showSlide(currentSlide);
        }

        function prevSlide() {
            currentSlide = (currentSlide - 1 + slides.length) % slides.length;
            showSlide(currentSlide);
        }

        // Show the first slide initially
        showSlide(currentSlide);
    </script>
</body>
</html>