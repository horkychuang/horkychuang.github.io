<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Class 3</title>
    <!-- MathJax Configuration -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <!-- Load MathJax -->
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <!-- Load Marked.js for Markdown parsing -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* General Styling */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background: #f4f4f9; /* Light Gray Background */
            color: #333;
        }

        /* Navigation Bar at TOP*/
        nav {
            background-color: #3498db; /* Blue Background */
            color: white;
            padding: 10px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        nav h1 {
            margin: 0;
            font-size: 24px;
        }
        nav ul {
            list-style: none;
            margin: 0;
            padding: 0;
            display: flex;
            gap: 20px;
        }
        nav ul li {
            display: inline;
        }
        nav ul li a {
            color: white;
            text-decoration: none;
            font-size: 18px;
            transition: color 0.3s ease;
        }
        nav ul li a:hover {
            color: #ecf0f1; /* Lighter White on Hover */
        }

        /* Section Styling */
        section {
            width: 80%;
            max-width: 900px;
            margin: 50px auto;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            text-align: left;
        }
        h1, h2, h3 {
            color: #34495e;
        }
        p, li {
            font-size: 18px;
            line-height: 1.6;
            color: #555;
        }
        pre {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 14px;
        }
        code {
            color: #e74c3c;
        }

        /* 兩種div的定義：Summary and Discussion */
        .summary {
            background-color: #ecf0f1;
            padding: 15px;
            border-left: 5px solid #3498db;
            text-align: left;
        }
        .discussion {
            background-color: #fef9e7;
            padding: 15px;
            border-left: 5px solid #f1c40f;
            text-align: left;
        }
    </style>
</head>
<body>

<!-- Navigation Bar -->
<nav>
    <h1>Class 3 Supervised Learning I</h1>
    <ul> <!-- NAV BAR在上面, 要跟下面的大 section們有連接 , -->
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#topic1">Type of Data</a></li>
        <li><a href="#topic2">KNN Algorithm</a></li>
        <li><a href="#topic3">Naive Bayes Algorithm</a></li>
    </ul>
</nav>

<!-- Introduction Section -->
 <section id="Introduction">
    <div class="discussion">
    <h1>Today's Topic</h1>
    <ol>
        <li>Basic for Supervised Learning</li>
        <li>Type of Data</li>
        <li>EZ Classification</li>
    </ol> 
</div>
</section>

<section id="introduction">
    <div class="summary">
    <h1>Introduction</h1>
    <p><small>
        A general notion of machine learning is to focus on the process of learning from data to discover hidden patterns or predict future events. 
        There are two <mark>basic</mark>> approaches within machine learning, namely, supervised learning and unsupervised learning. 
        The main difference is one uses labelled data to help predict outcomes while the other does not.
    </small></p>
    <h2>Supervised Learning</h2>
    <p><small>
       Supervised learning is the set of approaches that requires an explicit label containing the correct expected output for each row in the data through 
       which we want to learn. 
       These labels are either written by hand by domain experts or obtained from previous records or generated by software logs. 
       Such datasets are used to supervise algorithms into classifying the data or predicting outcomes 
    </small></p>
    <p><small>
    <mark>Classification</mark><br>
    Classification is a suite of supervised learning methods that aim to assign a discrete class label chosen out of a predefined limited set of options (two or more). 
    An example of such labels is a system that monitors the financial transactions and validates each transaction for any kind of fraud. 
    The past data is required, which contains regular transactions as well as fraudulent transactions explicitly stating which data item (or row) is fraudulent.  
    </small></p>
    <p><small>
    <mark>Regression</mark><br>
    Regression is a supervised learning technique that tries to capture the relationship between two variables. 
    We often have one or more independent variables, or the variables that we would always know, and we want to learn how to predict the value of a dependent variable. 
    In regression problems, we want to predict a continuous real value; that means the target value can have infinite possibilities unlike classification that has a selected few. 
    An example of regression is a system to predict the value of a stock the next day based on the value and volume traded on the previous day. 
    </small></p>
</div>
</section>


<section id="topic1">
<div class="summary">
    <h2>A. Types of Data</h2>
    <p><small>
       Data is a collection of facts, observations, measures, text, numbers, images, and videos. 
       It might be clean or unclean, ordered or unordered, having mixed data types, or completely pure and historical or real-time. 
       Data in itself is not useful till we clean it, arrange it, analyze it, and draw insights from it. 
    </small></p>
    <img src="images/2025072301.jpg" alt="2025072301" width="300">
    <p><small>
       <ul>
        <li><mark>Qualitative Data</mark> is the data type that cannot be measured or weighed, for example, taste, color, odor, fitness, name, etc. 
            Formally put, when we categorize something or make a classification for it</li>
            <ul>
                <li><mark>Binary data</mark> has only two classes that are mutually exclusive to each other. For example, Yes/No, dry/wet, hard/soft, good/bad, true/false, etc</li>
                <li><mark>Nominal data</mark> can be described as the type of data which though is categorized but does not have any sequence or order in it. For example, names, address, gender, etc., can be regarded as nominal data attributes. 
                    The only statistical central tendency that can be studied is the mode, or the quantity that occurs most often.</li>
                <li><mark>Ordinal data</mark> : we can order it in a sequence. For example, fast/medium/slow,positive/neutral/negative, T-shirt sizes (S, M, L, XL, etc.), 
                    Likert scale in customer surveys (Always, Sometimes, Rarely, Never) etc.</li>
                <li><mark>Interval data</mark>: Degrees Fahrenheit scale of temperatures, 
                    we can’t say that 60°F is twice as hot compared to 30°F.</li>
                <li><mark>Ratio data</mark>: with arithmetic operations of multiplication, division, etc.</li>
            </ul>
       </ul>
    </small></p>
    <pre><code>
import pandas as pd
df = pd.DataFrame([["Edward Remirez","Male",28,"Bachelors"],
                   ["Arnav Sharma","Male",23,"Masters"],
                   ["Sophia Smith","Female",19,"High School"]], 
                  columns=['Name','Gender','Age','Degree'])

### nominal attribute
from sklearn.preprocessing import OneHotEncoder
encoder_for_gender = OneHotEncoder().fit(df[['Gender']])
gender_values = encoder_for_gender.transform(df[['Gender']])
gender_values.toarray() # sparse matrix
df[['Gender_F', 'Gender_M']] = gender_values.toarray()
df

### can order it in a sequence
from sklearn.preprocessing import OrdinalEncoder
encoder_for_education = OrdinalEncoder()
encoder_for_education.fit_transform(df[['Degree']])
encoder_for_education.categories_

encoder_for_education = OrdinalEncoder(categories = [['Masters',  'Bachelors','High School', 'Doctoral']])
df[['Degree_encoded']] = encoder_for_education.fit_transform(df[['Degree']])
df   
    </code></pre>
</div>
</section>

<section>
<div class="discussion">
    <p><small>
       <ul>
        <li><mark>Quantitative data:</mark> All the types of data points which can be measured, weighed, scaled, recorded, etc. are quantitative. 
            For example, height, revenue, number of customers, demand quantity, area, volume, etc</li>
            <ul>
                <li><mark>Discrete data</mark> are precise, to-the-point, and integers. For example, the number of passengers in a plane or the population of a city cannot be in decimals</li>
                <li><mark>Continuous data</mark> can take any value, usually in a range. For example, height can take decimal values or the price of a product can need not be an integer</li>
                <ul>
                  <li><mark>Min-max scaling transforms</mark> : the minimum number in the dataset maps to zero and the maximum number maps to one</li>
                  <li><mark>Standard scaling transforms</mark>: removing the mean and scaling to unit variance. The value thus represents the z-value with respect to the mean and variance</li>
                </ul>
            </ul>
       </ul>
    </small></p>
</div>
</section>

<section>
<div class="summary">
    <h2>Structured data</h2>
    <p><small>
    dataset which can be represented in a row-column structure easily is a structured dataset.
    </small></p>
    <img src="images/2025072302.jpg" alt="2025072302" width="500">  
</div>
</section>


<section>
<div class="discussion">
    <h2>Unstructured data: Unstructured data can be text, audio, image, or video</h2>
    <img src="images/2025072303.jpg" alt="2025072303" width="400">
    <img src="images/2025072304.jpg" alt="2025072304" width="400">
</div>
</section>

<section>
<div class="discussion">
    <p><small>
       A vital aspect often ignored and less discussed is data quality. 
       Data quality determines the quality of the analysis and insights generated. Remember, garbage in, garbage out. 
    </small></p>
    <img src="images/2025070813.jpg" alt="2025070813" width="400">
</div>
</section>

<section >
<div class="summary">
    <h1>Performance for Classification</h1>
    <h2>Confusion Matrix</h2>
    <p><samll>
    <mark>Confusion matrix</mark> is a simple contingency table that is used to visualize the performance of a classification algorithm 
    which may classify the elements into two or more classes. 
    In the table, each row represents the items belonging to the actual classes, and each column represents the items belonging to the predicted classes
    </samll></p>
    <img src="images\churn_04.png" alt="churn_04" width="400">
    <p><small>
       The samples that are correctly labelled as negative are called True Negatives (TN), and the ones that are incorrectly labelled as negative are called False Negatives (FN). 
       Similarly, the model predicted eight people as positive, out of which five are correctly predicted as positive, thus indicating the True Positives (TP). 
       The three items that are incorrectly labelled as positive but are actually negative are called False Positives (FP).<br>
       True Positives and True Negatives amount to the overall accuracy of your model. 
       False Positives are often called Type 1 Error, and False Negatives are called Type 2 Error.
    </small></p>
</div>
</section>

<section>
<div class="discussion">
<h2 id='recall'>Recall</h2>
<p>Recall or sensitivity: Recall is how of all the actual positive events; how many we were able to capture. Recall is a measure that indicates the ratio of positive test data items that are correctly identified out of all the items that are actually positive</p>
<div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n4" cid="n4" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.837ex" height="4.812ex" role="img" focusable="false" viewBox="0 -1359 8768 2127" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.738ex;"><defs><path id="MJX-1-TEX-N-52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path id="MJX-1-TEX-N-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path id="MJX-1-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-1-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-1-TEX-N-6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path id="MJX-1-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-1-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-1-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-1-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-1-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path id="MJX-1-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="52" xlink:href="#MJX-1-TEX-N-52"></use><use data-c="65" xlink:href="#MJX-1-TEX-N-65" transform="translate(736,0)"></use><use data-c="63" xlink:href="#MJX-1-TEX-N-63" transform="translate(1180,0)"></use><use data-c="61" xlink:href="#MJX-1-TEX-N-61" transform="translate(1624,0)"></use><use data-c="6C" xlink:href="#MJX-1-TEX-N-6C" transform="translate(2124,0)"></use><use data-c="6C" xlink:href="#MJX-1-TEX-N-6C" transform="translate(2402,0)"></use></g><g data-mml-node="mo" transform="translate(2957.8,0)"><use data-c="3D" xlink:href="#MJX-1-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4013.6,0)"><g data-mml-node="mrow" transform="translate(1649.7,676)"><g data-mml-node="mi"><use data-c="1D447" xlink:href="#MJX-1-TEX-I-1D447"></use></g><g data-mml-node="mi" transform="translate(704,0)"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><use data-c="1D447" xlink:href="#MJX-1-TEX-I-1D447"></use></g><g data-mml-node="mi" transform="translate(704,0)"><use data-c="1D443" xlink:href="#MJX-1-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(1677.2,0)"><use data-c="2B" xlink:href="#MJX-1-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(2677.4,0)"><use data-c="1D439" xlink:href="#MJX-1-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(3426.4,0)"><use data-c="1D441" xlink:href="#MJX-1-TEX-I-1D441"></use></g></g><rect width="4514.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div></div>
<h2 id='precision'>Precision</h2>
<p>Precision is the measure that indicates ratio of the number of correctly predicted positive points to the number of all the points that were predicted as positive.</p>
<div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n7" cid="n7" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="22.439ex" height="4.812ex" role="img" focusable="false" viewBox="0 -1359 9918 2127" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.738ex;"><defs><path id="MJX-2-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path><path id="MJX-2-TEX-N-72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path><path id="MJX-2-TEX-N-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path id="MJX-2-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-2-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-2-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-2-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-2-TEX-N-6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path id="MJX-2-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-2-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-2-TEX-I-1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="50" xlink:href="#MJX-2-TEX-N-50"></use><use data-c="72" xlink:href="#MJX-2-TEX-N-72" transform="translate(681,0)"></use><use data-c="65" xlink:href="#MJX-2-TEX-N-65" transform="translate(1073,0)"></use><use data-c="63" xlink:href="#MJX-2-TEX-N-63" transform="translate(1517,0)"></use><use data-c="69" xlink:href="#MJX-2-TEX-N-69" transform="translate(1961,0)"></use><use data-c="73" xlink:href="#MJX-2-TEX-N-73" transform="translate(2239,0)"></use><use data-c="69" xlink:href="#MJX-2-TEX-N-69" transform="translate(2633,0)"></use><use data-c="6F" xlink:href="#MJX-2-TEX-N-6F" transform="translate(2911,0)"></use><use data-c="6E" xlink:href="#MJX-2-TEX-N-6E" transform="translate(3411,0)"></use></g><g data-mml-node="mo" transform="translate(4244.8,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(5300.6,0)"><g data-mml-node="mrow" transform="translate(1581.2,676)"><g data-mml-node="mi"><use data-c="1D447" xlink:href="#MJX-2-TEX-I-1D447"></use></g><g data-mml-node="mi" transform="translate(704,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mi"><use data-c="1D447" xlink:href="#MJX-2-TEX-I-1D447"></use></g><g data-mml-node="mi" transform="translate(704,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g><g data-mml-node="mo" transform="translate(1677.2,0)"><use data-c="2B" xlink:href="#MJX-2-TEX-N-2B"></use></g><g data-mml-node="mi" transform="translate(2677.4,0)"><use data-c="1D439" xlink:href="#MJX-2-TEX-I-1D439"></use></g><g data-mml-node="mi" transform="translate(3426.4,0)"><use data-c="1D443" xlink:href="#MJX-2-TEX-I-1D443"></use></g></g><rect width="4377.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div></div>
<h2 id='accuracy'>Accuracy</h2>
<p>Accuracy is how many predictions were made correctly. Accuracy is a simple measure that denotes how many items are correctly classified into both the classes.</p>
<p>&nbsp;</p>
<h2 id='f-measure'>F-measure</h2>
<p>F-measure or F1-Score is a score obtained by taking the harmonic mean of precision and recall to give a general picture of the goodness of the classification model. F1 score is the harmonic mean of precision and recall. The higher the F1 score, the better.</p>
<div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n13" cid="n13" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="34.655ex" height="4.837ex" role="img" focusable="false" viewBox="0 -1370 15317.4 2138" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.738ex;"><defs><path id="MJX-3-TEX-N-46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"></path><path id="MJX-3-TEX-N-2D" d="M11 179V252H277V179H11Z"></path><path id="MJX-3-TEX-N-4D" d="M132 622Q125 629 121 631T105 634T62 637H29V683H135Q221 683 232 682T249 675Q250 674 354 398L458 124L562 398Q666 674 668 675Q671 681 683 682T781 683H887V637H854Q814 636 803 634T785 622V61Q791 51 802 49T854 46H887V0H876Q855 3 736 3Q605 3 596 0H585V46H618Q660 47 669 49T688 61V347Q688 424 688 461T688 546T688 613L687 632Q454 14 450 7Q446 1 430 1T410 7Q409 9 292 316L176 624V606Q175 588 175 543T175 463T175 356L176 86Q187 50 261 46H278V0H269Q254 3 154 3Q52 3 37 0H29V46H46Q78 48 98 56T122 69T132 86V622Z"></path><path id="MJX-3-TEX-N-65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path id="MJX-3-TEX-N-61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path id="MJX-3-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-3-TEX-N-75" d="M383 58Q327 -10 256 -10H249Q124 -10 105 89Q104 96 103 226Q102 335 102 348T96 369Q86 385 36 385H25V408Q25 431 27 431L38 432Q48 433 67 434T105 436Q122 437 142 438T172 441T184 442H187V261Q188 77 190 64Q193 49 204 40Q224 26 264 26Q290 26 311 35T343 58T363 90T375 120T379 144Q379 145 379 161T380 201T380 248V315Q380 361 370 372T320 385H302V431Q304 431 378 436T457 442H464V264Q464 84 465 81Q468 61 479 55T524 46H542V0Q540 0 467 -5T390 -11H383V58Z"></path><path id="MJX-3-TEX-N-72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-3-TEX-N-70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z"></path><path id="MJX-3-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-3-TEX-N-69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path id="MJX-3-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-3-TEX-N-6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path id="MJX-3-TEX-N-52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z"></path><path id="MJX-3-TEX-N-6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"></path><path id="MJX-3-TEX-N-50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z"></path><path id="MJX-3-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><use data-c="46" xlink:href="#MJX-3-TEX-N-46"></use><use data-c="2D" xlink:href="#MJX-3-TEX-N-2D" transform="translate(653,0)"></use><use data-c="4D" xlink:href="#MJX-3-TEX-N-4D" transform="translate(986,0)"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(1903,0)"></use><use data-c="61" xlink:href="#MJX-3-TEX-N-61" transform="translate(2347,0)"></use><use data-c="73" xlink:href="#MJX-3-TEX-N-73" transform="translate(2847,0)"></use><use data-c="75" xlink:href="#MJX-3-TEX-N-75" transform="translate(3241,0)"></use><use data-c="72" xlink:href="#MJX-3-TEX-N-72" transform="translate(3797,0)"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(4189,0)"></use></g><g data-mml-node="mo" transform="translate(4910.8,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(5966.6,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(722.2,0)"><use data-c="2217" xlink:href="#MJX-3-TEX-N-2217"></use></g><g data-mml-node="mtext" transform="translate(1444.4,0)"><use data-c="70" xlink:href="#MJX-3-TEX-N-70"></use><use data-c="72" xlink:href="#MJX-3-TEX-N-72" transform="translate(556,0)"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(948,0)"></use><use data-c="63" xlink:href="#MJX-3-TEX-N-63" transform="translate(1392,0)"></use><use data-c="69" xlink:href="#MJX-3-TEX-N-69" transform="translate(1836,0)"></use><use data-c="73" xlink:href="#MJX-3-TEX-N-73" transform="translate(2114,0)"></use><use data-c="69" xlink:href="#MJX-3-TEX-N-69" transform="translate(2508,0)"></use><use data-c="6F" xlink:href="#MJX-3-TEX-N-6F" transform="translate(2786,0)"></use><use data-c="6E" xlink:href="#MJX-3-TEX-N-6E" transform="translate(3286,0)"></use></g><g data-mml-node="mo" transform="translate(5508.7,0)"><use data-c="2217" xlink:href="#MJX-3-TEX-N-2217"></use></g><g data-mml-node="mtext" transform="translate(6230.9,0)"><use data-c="52" xlink:href="#MJX-3-TEX-N-52"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(736,0)"></use><use data-c="63" xlink:href="#MJX-3-TEX-N-63" transform="translate(1180,0)"></use><use data-c="61" xlink:href="#MJX-3-TEX-N-61" transform="translate(1624,0)"></use><use data-c="6C" xlink:href="#MJX-3-TEX-N-6C" transform="translate(2124,0)"></use><use data-c="6C" xlink:href="#MJX-3-TEX-N-6C" transform="translate(2402,0)"></use></g></g><g data-mml-node="mrow" transform="translate(740.7,-686)"><g data-mml-node="mtext"><use data-c="50" xlink:href="#MJX-3-TEX-N-50"></use><use data-c="72" xlink:href="#MJX-3-TEX-N-72" transform="translate(681,0)"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(1073,0)"></use><use data-c="63" xlink:href="#MJX-3-TEX-N-63" transform="translate(1517,0)"></use><use data-c="69" xlink:href="#MJX-3-TEX-N-69" transform="translate(1961,0)"></use><use data-c="73" xlink:href="#MJX-3-TEX-N-73" transform="translate(2239,0)"></use><use data-c="69" xlink:href="#MJX-3-TEX-N-69" transform="translate(2633,0)"></use><use data-c="6F" xlink:href="#MJX-3-TEX-N-6F" transform="translate(2911,0)"></use><use data-c="6E" xlink:href="#MJX-3-TEX-N-6E" transform="translate(3411,0)"></use></g><g data-mml-node="mo" transform="translate(4189.2,0)"><use data-c="2B" xlink:href="#MJX-3-TEX-N-2B"></use></g><g data-mml-node="mtext" transform="translate(5189.4,0)"><use data-c="52" xlink:href="#MJX-3-TEX-N-52"></use><use data-c="65" xlink:href="#MJX-3-TEX-N-65" transform="translate(736,0)"></use><use data-c="63" xlink:href="#MJX-3-TEX-N-63" transform="translate(1180,0)"></use><use data-c="61" xlink:href="#MJX-3-TEX-N-61" transform="translate(1624,0)"></use><use data-c="6C" xlink:href="#MJX-3-TEX-N-6C" transform="translate(2124,0)"></use><use data-c="6C" xlink:href="#MJX-3-TEX-N-6C" transform="translate(2402,0)"></use></g></g><rect width="9110.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></div></div>
<p>&nbsp;</p>
</div>
</section>


<section >
<div class="summary">
    <p><small>
        ROC curve and AUC value: ROC or receiver operating characteristics is used to compare different models. 
        It is a plot between TPR (true positive rate) and FPR (false positive rate). 
        The area under the ROC curve (AUC) is a measure of how good a model is. The higher the AUC values, the better the model,
    </small></p>
    <img src="images\2025070907.jpg" alt="2025070907" width="400">
    <p><small>
        <mark>Classification Report</mark><br>
        Classification report gives most of the important and common metrics required for classification tasks in one single view.
    </small></p>
</div>
</section>

<section>
<div class="discussion">
    <h2>Cross Validation</h2>
    <p><small>
        We divide the labelled dataset into two components – namely, <mark>training set</mark> and <mark>validation (or testing) set</mark>. 
        Rather than relying on a static part of the data for learning the model and using other static part for validation, 
        it is a good idea to come with a rotation of training and testing parts to be able to determine how well will the model generalize to an independent dataset.
    </small></p>
    <img src="images/2025061306.jpg" alt="2025061306" width="400">
    <P><small>Over the k iterations, we will obtain k metrics, 
        which can be averaged to find a more generalizable metric that can be used to tune the hyperparameters</small></P>
    <h2>Hyperparameter Tuning</h2>
    <p><small>
       While approaching a machine learning problem, you have to engineer and select the right features, 
       pick the algorithm, and tune the selected algorithm (or algorithms) for the hyperparameters they are affected by. 
    </small></p>
</div>
</section>

<section id="topic2">
<div class="summary">
    <h1>EZ Classification</h1>
    <h2>K Nearest Neighbors (KNN)</h2>
    <P><small>
       "Birds of a feather flock together". 
       Data science uses this principle to classify data by placing it in the same category as <mark>similar</mark> or <mark>k nearest</mark> neighbors. 
       `K nearest neighbors` is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). It is a lazy learning.  
    </small></P>
    <h2>KNN Method</h2>
    <p><small>
       The letter k is a variable term implying that any number of nearest neighbors could be used. 
       In particular, the "nearest" in similarity. The k-NN algorithm treats the features as coordinates in a multidimensional feature space.<br>
       If we use the k-NN algorithm with k = 3 instead, it performs a vote among the three nearest neighbors: orange, grape, and nuts. 
       Since the majority class among these neighbors is fruit (two of the three votes), the tomato again is classified as a fruit.
       Traditionally, the k-NN algorithm uses <mark>Euclidean distance</mark>, which is the distance one would measure 
       if it were possible to use a ruler to connect two points. 
    </small></p>
    <img src="images\ch3-1.png" alt="ch3-1" width="400">
</div>
</section>


<section >
<div class="summary">
    <h2>Appropriate k</h2>
    <p><small>
        The balance between over fitting and under fitting the training data is a problem known as <mark>bias-variance tradeoff</mark>. 
        Choosing a large k reduces the impact or variance caused by noisy data, but can bias the learner.
    </small></p>
    <img src="images\ch3-2.png" alt="ch3-2" width="400">
    <p><small>
       <ul>
        <li>Choosing the optimal value for K is best done by first inspecting the data. 
            In general, a large K value is more precise as it reduces the overall noise but there is no guarantee</li>
        <li>Cross-validation is another way to retrospectively determine a good K value by using an independent dataset to validate the K value</li>
        <li>Historically, the optimal K for most datasets has been between 3-10</li>
       </ul>
    </small></p>
</div>
</section>

<section id="topic2">
<div class="discussion">
    <h2>A Simple Example</h2>
    <p><small>
       Let's consider the following data concerning credit default. 
       Age and Loan are two `numerical variables` (predictors) and Default is the target. 
       We can now use the training set to classify an unknown case `(Age=48 and Loan=$142,000)` using Euclidean distance. 
    </small></p>
    <img src="images\ch3-3.png" alt="ch3-3" width="400">
</div>
</section>

<section >
<div class="summary">
    <h1>Standardized Distance</h1>
    <p><small>
       One major drawback in calculating distance measures directly from the training set is in the case 
       where variables have different measurement scales or there is a mixture of numerical and categorical variables.<br>
       For example, if one variable is based on annual income in dollars, and the other is based on age in years 
       then income will have a much higher influence on the distance calculated.  
       One solution is to standardize the training set as shown below.
    </small></p>
    <img src="images\ch3-4.png" alt="ch3-4" width="400">
    <h2>Why is the k-NN Algorithm Lazy?</h2>
    <p><small>
        A lazy learner is not really learning anything. Due to the heavy reliance on the training instances rather than an abstracted model, 
        lazy learning is also known as instance-based learning or rote learning. 
        As <mark>instance-based learners</mark> do not build a model, 
        the method is said to be in a class of non-parametric learning methods—no parameters are learned about the data.
    </small></p>
</div>
</section>

<section >
<div class="discussion">
    <h2>Python Example</h2>
    <pre><code>
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
from sklearn.inspection import permutation_importance

# Step 1: Generate a dataset with 356 rows and 8 features
X, y = make_classification(n_samples=356, n_features=8, 
                          n_informative=6, n_redundant=2, 
                          n_classes=2, random_state=42)

# Convert to DataFrame (optional, for better visualization)
df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(8)])
df['Target'] = y

print("Dataset shape:", df.shape)
print("\nFirst 5 rows of the dataset:")
print(df.head())

# Step 2: Prepare features and target
X = df.iloc[:, :-1]  # First 8 columns (features)
y = df['Target']     # Last column (target)

# Step 3: Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 4: Standardize the features (important for KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Train the KNN model
k = 5  # Number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train_scaled, y_train)

# Feature importance using permutation importance
perm_importance = permutation_importance(knn, X_train_scaled, y_train, n_repeats=10, random_state=42)
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': perm_importance.importances_mean,
    'Std': perm_importance.importances_std
})

# Sort by importance
significant_features = feature_importance.sort_values(by='Importance', ascending=False)

print("\nFeature Importance (Permutation Importance):")
print(significant_features)

# Step 6: Make predictions
y_pred = knn.predict(X_test_scaled)

# Step 7: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix (Training)
y_train_pred = knn.predict(X_train_scaled)
cm_train = confusion_matrix(y_train, y_train_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Training)')
plt.show()

# Training Accuracy and AUC
y_train_pred_prob = knn.predict_proba(X_train_scaled)[:, 1]
train_accuracy = accuracy_score(y_train, y_train_pred)
train_auc = roc_auc_score(y_train, y_train_pred_prob)
print(f"\nTraining Accuracy: {train_accuracy:.4f}")
print(f"Training AUC: {train_auc:.4f}")

# Evaluate on test data
y_test_pred_prob = knn.predict_proba(X_test_scaled)[:, 1]
y_test_pred = knn.predict(X_test_scaled)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_auc = roc_auc_score(y_test, y_test_pred_prob)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {test_auc:.4f}")

# ROC Curve (Training and Test)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_prob)
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr_train, tpr_train, label=f'Training (AUC = {train_auc:.4f})')
plt.plot(fpr_test, tpr_test, label=f'Test (AUC = {test_auc:.4f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

# Optional: Try different values of k to find the best one
print("\n--- Testing different values of k ---")
k_range = range(1, 11)
scores = {}
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    y_pred_k = knn.predict(X_test_scaled)
    scores[k] = accuracy_score(y_test, y_pred_k)
    print(f"k={k}, Accuracy={scores[k]:.4f}")
    </code></pre>
</div>
</section>

<section id="topic3">
<div class="summary">
    <h1>Naive Bayes Algorithm</h1>
    <p><small>
       Naive Bayes is a category of Bayesian classifiers that aim to predict the probability 
       that a given data point belongs to a particular class. 
       These methods are based on Bayes’ theorem, which is one of the fundamental theorems in probabilities. 
       Naive Bayes classifiers have a general assumption that the effect of an attribute value on a given class in independent of the values of the other attributes. 
       This assumption is called <mark>class-conditional independence</mark>. It simplifies the computations involved and thus considered naive.
    </small></p>
    <h2>Bayes Theorem</h2>
    <div style="text-align: center; margin: 20px 0;">
            $$
            P(Y|X) = \frac{P(X|Y) \cdot P(Y)}{P(X)}
            $$
    </div>
        <p><small>
            Where:
            <ul>
                <li>$P(Y)$: Prior probability of class $Y$</li>
                <li>$P(X|Y)$: Likelihood (class-conditional probability)</li>
                <li>$P(X)$: Evidence (normalizing constant)</li>
            </ul>
        </small></p>
    
</div>
</section>


<section>
<div class="discussion">
    <h2>Conditional Probability</h2>
    <div style="text-align: center; margin: 20px 0;">
            $$
            P(A|B)=\frac{P(A\cap B)}{P(B)}
            $$
    </div>
    <p><small>
       Rearranging this formula once more with the knowledge to obtain the <mark>posterior probability</mark> 
       based on the prior probability $p(A)$ and likelihood $p\left(B\left|A\right.\right)$ 
    </small></p>
    <div style="text-align: center; margin: 20px 0;">
            $$
            p\left(A\left|B\right.\right)=\frac{p\left(A\cap B\right)}{p\left(B\right)}=\frac{p\left(B\left|A\right.\right)p\left(A\right)}{p(B)}
            $$
    </div>
 </div>
</section>

<section>
<div class="discussion">
    <h2>A Simple Example</h2>
    <p><small>
       Suppose that our training set consists of 20 instances, each recording the value of four attributes as well as the classification.  
       We will use classifications: `cancelled`, `very late`, `late` and `on time` to correspond to the events E1, E2, E3 and E4 described previously.
    </small></p>
    <img src="images\ch5-1.png" alt="ch5-1" width="400"> 
    <p><small>
        we want to calculate the probability of P (class = on time | day = weekday and season = winter and wind = high and rain = heavy) 
        and do similarly for the other three possible classifications (cancelled, very late, late ) <br>
        We could start by using conditional probabilities based on a `single` attribute
        <ul>
            <li>P (class = on time | season = winter) = 2/6 = 0.33</li>
            <li>P (class = late | season = winter) = 1/6 = 0.17</li>
            <li>P (class = very late | season = winter) = 3/6 = 0.5</li>
            <li>P (class = cancelled | season = winter) = 0/6 = 0.</li>
        </ul>
        If we have many information such as $E_1, E_2, E_3, E_4$, then
    </small></p>
    <div style="text-align: center; margin: 20px 0;">
        $$
        p\left(\textrm{on time}\left|E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right.\right)=\frac{p\left(E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\left|\textrm{on time}\right.\right)p\left(\textrm{on time}\right)}{p\left(E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right)}
        $$
    </div>
    <P><small>
       This formula is computationally difficult to solve. 
       Specifically, it assumes class-conditional independence, which means that events are Independent so long as they are conditioned on the same class value. 
    </small></P>
    <div style="text-align: center; margin: 20px 0;">
        $$
        p\left(\textrm{on time}\left|E_{1}\cap E_{2}\cap E_{3}\cap E_{4}\right.\right)\propto\left[\sum_{i=1}^{4}p\left(E_{i}\left|\textrm{on time}\right.\right)\right]p\left(\textrm{on time}\right)
        $$
    </div>
</div>
</section>

<section >
<div class="summary">
    <p><small>
       The Naive Bayes algorithm gives us a way of combining the `prior probability` and `conditional probabilities` 
       in a single formula to calculate the probability of each of the possible classifications in turn. 
    </small></p>
    <img src="images\ch5-3.png" alt="ch5-3" width="400"> 
    <p><small>
       If there is a `ZERO conditional probability`, i.e. $p\left(E_{i}\left|\textrm{on time}\right.\right)=0$, then the above is over. 
       The <mark>Laplace estimator</mark> is a simple yet powerful technique in Naive Bayes 
       that ensures all feature values have a non-zero probability by adding 1 to each count. 
    </small></p>
</div>
</section>

<section id="topic">
<div class="discussion">
    <h2>Python Example</h2>
    <pre><code>
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
from sklearn.datasets import make_classification

# Step 1: Generate a dataset with 356 rows and 8 features
# Binary classification (2 classes)
X, y = make_classification(
    n_samples=356,
    n_features=8,
    n_informative=6,
    n_redundant=2,
    n_classes=2,
    random_state=42
)

# Convert to DataFrame for better readability (optional)
df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(8)])
df['Target'] = y

print("Dataset shape:", df.shape)
print("\nFirst 5 rows of the dataset:")
print(df.head())

# Step 2: Define features (X) and target (y)
X = df.iloc[:, :-1]  # First 8 columns (features)
y = df['Target']     # Last column (target)

# Step 3: Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 4: Initialize and train the Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Step 5: Make predictions
y_pred = nb_model.predict(X_test)

# Step 6: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.4f}")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Optional: Show predicted probabilities for the first few test samples
y_prob = nb_model.predict_proba(X_test)
print("\nPredicted probabilities (first 5 test samples):")
print(pd.DataFrame(y_prob, columns=['P(Class=0)', 'P(Class=1)']).head())

# Training Accuracy and AUC
y_train_pred = nb_model.predict(X_train)
y_train_pred_prob = nb_model.predict_proba(X_train)[:, 1]
train_accuracy = accuracy_score(y_train, y_train_pred)
train_auc = roc_auc_score(y_train, y_train_pred_prob)
print(f"\nTraining Accuracy: {train_accuracy:.4f}")
print(f"Training AUC: {train_auc:.4f}")

# Test Accuracy and AUC
y_test_pred = y_pred  # Already computed
y_test_pred_prob = nb_model.predict_proba(X_test)[:, 1]
test_accuracy = accuracy_score(y_test, y_test_pred)
test_auc = roc_auc_score(y_test, y_test_pred_prob)
print(f"\nTest Accuracy: {test_accuracy:.4f}")
print(f"Test AUC: {test_auc:.4f}")

# ROC Curve (Training and Test)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_prob)
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr_train, tpr_train, label=f'Training (AUC = {train_auc:.4f})')
plt.plot(fpr_test, tpr_test, label=f'Test (AUC = {test_auc:.4f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()
    </code></pre>
</div>
</section>

<section >
<div class="summary">
    <h2>Application: Customer Churn Analysis</h2>
    <p><small>
       When you are running a shop or selling a service there is nothing more important than the customer. To understand how to best `retain customers`, 
       it is necessary to look at leaving customers and investigate their reasons for leaving. This is the domain of customer churn analysis 
    </small></p>
    <img src="images\churn_01.png" alt="churn_01" width="400">
    <p><small>
       Define User Churn
       <ul>
         <li>Users have the ability to visit the `page` *“Submit Downgrade”* and then either remain in a paid or free service</li>
         <li>Whether `customers have canceled` within the last month — the column is called “Churn”</li>
         <li>This Churn occurs due to leakage in revenue generation models, such as</li>
         <ul>
            <li>Payment failure or card declined by a bank</li>
            <li>Hard declines due to a stolen or lost card</li>
            <li>Using an expired card</li>
         </ul>
       </ul>
    </small></p>
</div>
</section>

<section>
<div class="discussion">
    <h2>Python Example</h2>
    <pre><code>
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic dataset with 10,000 rows and 8 features
n_samples = 10000

# Feature 1: Age (18-70)
age = np.random.normal(40, 12, n_samples)
age = np.clip(age, 18, 70)

# Feature 2: Monthly Charges (20-150)
monthly_charges = np.random.normal(65, 30, n_samples)
monthly_charges = np.clip(monthly_charges, 20, 150)

# Feature 3: Tenure (0-72 months)
tenure = np.random.exponential(24, n_samples)
tenure = np.clip(tenure, 0, 72)

# Feature 4: Contract Type (0: Month-to-month, 1: One year, 2: Two year)
contract_type = np.random.choice([0, 1, 2], n_samples, p=[0.5, 0.3, 0.2])

# Feature 5: Internet Service (0: No, 1: DSL, 2: Fiber optic)
internet_service = np.random.choice([0, 1, 2], n_samples, p=[0.1, 0.4, 0.5])

# Feature 6: Payment Method (0: Electronic check, 1: Mailed check, 2: Bank transfer, 3: Credit card)
payment_method = np.random.choice([0, 1, 2, 3], n_samples, p=[0.35, 0.25, 0.2, 0.2])

# Feature 7: Number of Complaints
num_complaints = np.random.poisson(1.2, n_samples)
num_complaints = np.clip(num_complaints, 0, 5)

# Feature 8: Data Usage (GB per month)
data_usage = np.random.gamma(2, 10, n_samples)
data_usage = np.clip(data_usage, 0, 100)

# Create the DataFrame
df = pd.DataFrame({
    'Age': age.astype(int),
    'MonthlyCharges': np.round(monthly_charges, 2),
    'Tenure': tenure.astype(int),
    'ContractType': contract_type,
    'InternetService': internet_service,
    'PaymentMethod': payment_method,
    'NumComplaints': num_complaints,
    'DataUsage': np.round(data_usage, 2)
})

# Create mapping for categorical variables for better readability
contract_map = {0: 'Month-to-month', 1: 'One year', 2: 'Two year'}
internet_map = {0: 'No', 1: 'DSL', 2: 'Fiber optic'}
payment_map = {0: 'Electronic check', 1: 'Mailed check', 2: 'Bank transfer', 3: 'Credit card'}

# Create a readable version of the dataframe
df_readable = df.copy()
df_readable['ContractTypeLabel'] = df_readable['ContractType'].map(contract_map)
df_readable['InternetServiceLabel'] = df_readable['InternetService'].map(internet_map)
df_readable['PaymentMethodLabel'] = df_readable['PaymentMethod'].map(payment_map)

# Generate the target variable (Churn) based on logical relationships with the features
# Customers are more likely to churn if:
# - They have month-to-month contracts
# - They have high monthly charges
# - They have low tenure
# - They have fiber optic internet (higher expectations)
# - They use electronic check payment
# - They have multiple complaints
# - They are younger

churn_prob = (
    0.1 + 
    0.3 * (df['ContractType'] == 0) +  # Month-to-month contracts more likely to churn
    0.2 * (df['MonthlyCharges'] > 80) +
    0.2 * (df['Tenure'] < 12) +
    0.15 * (df['InternetService'] == 2) +  # Fiber optic users more likely to churn
    0.15 * (df['PaymentMethod'] == 0) +  # Electronic check users more likely to churn
    0.1 * (df['Age'] < 30) +
    0.25 * (df['NumComplaints'] > 1) +
    0.1 * np.random.random(n_samples)  # Add some randomness
)

# Ensure probabilities are between 0 and 1
churn_prob = np.clip(churn_prob, 0, 1)

# Generate churn based on probabilities
churn = np.random.binomial(1, churn_prob, n_samples)

# Add churn to the dataframe
df['Churn'] = churn
df_readable['Churn'] = churn

# Display basic dataset information
print("Dataset Shape:", df.shape)
print("\nFirst few rows of the dataset:")
print(df_readable.head())

print("\nDataset Info:")
print(df.info())

print("\nChurn Distribution:")
print(df['Churn'].value_counts())
print(f"Churn Rate: {df['Churn'].mean():.2%}")

# Exploratory Data Analysis
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
fig.suptitle('Customer Churn Analysis - Feature Distributions', fontsize=16)

# Age vs Churn
df.boxplot(column='Age', by='Churn', ax=axes[0,0])
axes[0,0].set_title('Age Distribution by Churn')

# Monthly Charges vs Churn
df.boxplot(column='MonthlyCharges', by='Churn', ax=axes[0,1])
axes[0,1].set_title('Monthly Charges by Churn')

# Tenure vs Churn
df.boxplot(column='Tenure', by='Churn', ax=axes[0,2])
axes[0,2].set_title('Tenure by Churn')

# Contract Type vs Churn
contract_churn = pd.crosstab(df['ContractType'], df['Churn'], normalize='index')
contract_churn.plot(kind='bar', ax=axes[0,3], title='Churn Rate by Contract Type')
axes[0,3].set_xlabel('Contract Type')
axes[0,3].set_ylabel('Churn Rate')

# Internet Service vs Churn
internet_churn = pd.crosstab(df['InternetService'], df['Churn'], normalize='index')
internet_churn.plot(kind='bar', ax=axes[1,0], title='Churn Rate by Internet Service')
axes[1,0].set_xlabel('Internet Service')
axes[1,0].set_ylabel('Churn Rate')

# Payment Method vs Churn
payment_churn = pd.crosstab(df['PaymentMethod'], df['Churn'], normalize='index')
payment_churn.plot(kind='bar', ax=axes[1,1], title='Churn Rate by Payment Method')
axes[1,1].set_xlabel('Payment Method')
axes[1,1].set_ylabel('Churn Rate')

# Number of Complaints vs Churn
complaints_churn = pd.crosstab(df['NumComplaints'], df['Churn'], normalize='index')
complaints_churn.plot(kind='bar', ax=axes[1,2], title='Churn Rate by Number of Complaints')
axes[1,2].set_xlabel('Number of Complaints')
axes[1,2].set_ylabel('Churn Rate')

# Data Usage vs Churn
df.boxplot(column='DataUsage', by='Churn', ax=axes[1,3])
axes[1,3].set_title('Data Usage by Churn')

plt.tight_layout()
plt.show()

# Prepare data for modeling
X = df.drop('Churn', axis=1)
y = df['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create and train the Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred = nb_model.predict(X_test)
y_pred_proba = nb_model.predict_proba(X_test)[:, 1]

# Model Evaluation
print("\n" + "="*50)
print("MODEL EVALUATION - NAIVE BAYES")
print("="*50)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['No Churn', 'Churn'], 
            yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# Feature Importance (using Naive Bayes feature log probabilities)
# Calculate the difference in means between churn and non-churn for each feature
feature_importance = []
for i, feature in enumerate(X.columns):
    churn_mean = X_train[y_train == 1][feature].mean()
    non_churn_mean = X_train[y_train == 0][feature].mean()
    diff = abs(churn_mean - non_churn_mean)
    feature_importance.append(diff)

# Normalize feature importance
feature_importance = np.array(feature_importance)
feature_importance = feature_importance / feature_importance.sum()

# Plot feature importance
plt.figure(figsize=(10, 6))
indices = np.argsort(feature_importance)[::-1]
plt.title('Feature Importance (based on mean differences)')
plt.bar(range(len(feature_importance)), feature_importance[indices])
plt.xticks(range(len(feature_importance)), [X.columns[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()

# Print feature importance
print("\nFeature Importance:")
for i in indices:
    print(f"{X.columns[i]}: {feature_importance[i]:.3f}")

# Predict churn probability for new customers
print("\n" + "="*50)
print("PREDICTION EXAMPLES")
print("="*50)

# Create a few sample customers
sample_customers = pd.DataFrame({
    'Age': [25, 45, 35, 60],
    'MonthlyCharges': [100, 50, 80, 60],
    'Tenure': [2, 36, 12, 48],
    'ContractType': [0, 2, 0, 1],
    'InternetService': [2, 1, 2, 0],
    'PaymentMethod': [0, 2, 0, 3],
    'NumComplaints': [3, 0, 1, 0],
    'DataUsage': [80, 40, 60, 10]
})

# Predict churn probability for sample customers
sample_pred_proba = nb_model.predict_proba(sample_customers)[:, 1]
sample_pred = nb_model.predict(sample_customers)

print("Sample Customer Predictions:")
for i in range(len(sample_customers)):
    status = "Churn" if sample_pred[i] == 1 else "No Churn"
    print(f"Customer {i+1}: {status} (Probability: {sample_pred_proba[i]:.3f})")

# Summary of findings
print("\n" + "="*50)
print("SUMMARY AND RECOMMENDATIONS")
print("="*50)
print(f"1. Dataset contains {len(df)} customers with {df['Churn'].sum()} churn cases ({df['Churn'].mean():.1%}).")
print(f"2. The Naive Bayes model achieved an accuracy of {accuracy_score(y_test, y_pred):.1%}.")
print(f"3. Key factors influencing churn:")
top_features = [X.columns[i] for i in indices[:3]]
for i, feature in enumerate(top_features, 1):
    print(f"   {i}. {feature}")

print(f"\n4. Recommendations:")
print(f"   - Focus retention efforts on customers with {top_features[0].lower()} characteristics")
print(f"   - Develop special offers for customers with high {top_features[1].lower()}")
print(f"   - Improve service for customers exhibiting {top_features[2].lower()} patterns") 
    </code></pre>
</div>
</section>




</body>
</html>