{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating NumPy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Array Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D: Monthlysales for North America (in thousands USD)\n",
    "monthly_na_sales_kusd = np.array([120.5, 135.2, 140.0, 130.8, 145.6])\n",
    "print(f\" Loaded 1D time series: {len(monthly_na_sales_kusd)} months of NA sales\")\n",
    "\n",
    "print(f\"\\n Monthly Sales (North America, K USD): {monthly_na_sales_kusd}\")\n",
    "print(f\"1D Monthly Sales | Shape: {monthly_na_sales_kusd.shape} | Dim: {monthly_na_sales_kusd.ndim} | Size: {monthly_na_sales_kusd.size}\")\n",
    "\n",
    "# 2D: Sales by Region × Product Category\n",
    "sales_by_region_product_kusd = np.array([\n",
    "    [250, 80],   # North: [Laptops, Accessories]\n",
    "    [180, 95]    # South: [Laptops, Accessories]\n",
    "])\n",
    "regions = [\"North\", \"South\"]\n",
    "products = [\"Laptops\", \"Accessories\"]\n",
    "\n",
    "print(\" Built 2D sales matrix: Region vs Product\")\n",
    "print(f\"\\n Sales by Region & Product (K USD):\\n{sales_by_region_product_kusd}\")\n",
    "print(f\"2D Sales Matrix | Shape: {sales_by_region_product_kusd.shape} | Dim: {sales_by_region_product_kusd.ndim} | Size: {sales_by_region_product_kusd.size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Special ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrices\n",
    "zeros_template = np.zeros((2, 5))  # Blank report template\n",
    "identity_matrix = np.identity(3)   # For financial model stability checks\n",
    "\n",
    "# ndarray with np.ones\n",
    "np.ones(shape=(2,2))\n",
    "\n",
    "# ndarray with np.arange\n",
    "np.arange(5)\n",
    "\n",
    "# an ndarray with np.random.randn that \"not need to reshape\"\n",
    "np.random.randn(2,2)\n",
    "\n",
    "# Evenly spaced revenue targets over 50 periods (planning)\n",
    "revenue_targets = np.linspace(100, 500, num=50)\n",
    "\n",
    "# Random daily customer flow simulation\n",
    "customer_flow = np.arange(0.5, 10.4, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planning_periods = 50\n",
    "revenue_targets_kusd = np.linspace(100, 500, num=planning_periods)\n",
    "print(f\" Generated {planning_periods} revenue targets for strategic planning\")\n",
    "\n",
    "# Blank templates for report initialization\n",
    "blank_performance_template = np.zeros((2, 5))  # e.g., 2 teams, 5 KPIs\n",
    "model_stability_matrix = np.eye(3)  # Identity for model validation\n",
    "print(\" Initialized blank templates and identity matrix for QA\")\n",
    "\n",
    "np.array([-1, 0, 1], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using Eyes Method and random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes_array = np.eye(7)\n",
    "print(eyes_array)\n",
    "\n",
    "# Uniform random (e.g., sensitivity testing)\n",
    "sensitivity_min, sensitivity_max = -3.4, 5.9\n",
    "sensitivity_noise = (sensitivity_max - sensitivity_min) * np.random.random_sample((3, 4)) + sensitivity_min\n",
    "print(f\"\\n Sensitivity Noise (Range [{sensitivity_min}, {sensitivity_max}]):\\n{np.round(sensitivity_noise, 2)}\")\n",
    "\n",
    "# Daily demand simulation (Normal dist: mean=300, std=50)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "simulated_daily_demand = np.random.randn(4, 5) * 50 + 300\n",
    "print(f\"\\n Simulated Daily Demand (units):\\n{simulated_daily_demand.astype(int)}\")\n",
    "print(\"Simulated 4 weeks of demand for inventory planning\")\n",
    "\n",
    "# Random order sizes\n",
    "order_quantities = np.random.randint(20, 100, size=5)\n",
    "print(f\" Simulated Order Sizes: {order_quantities}\")\n",
    "print(\"Generated random order sizes for procurement testing\")\n",
    "\n",
    "# random integers between a certain range\n",
    "integer_random = np.random.randint(5, 50, 5)\n",
    "print (integer_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D: Simulated revenue cube (Year × Quarter × Region × Product)\n",
    "# Structure: (2 years, 2 quarters/year, 2 regions, 2 products)\n",
    "revenue_cube_kusd = np.arange(16).reshape((2, 2, 2, 2)) * 10 + 100\n",
    "print(\" Initialized 4D revenue cube for multi-year forecasting\")\n",
    "\n",
    "print(f\"\\n 4D Revenue Cube Shape: {revenue_cube_kusd.shape}\")\n",
    "print(f\"4D Revenue Cube | Shape: {revenue_cube_kusd.shape} | Dim: {revenue_cube_kusd.ndim} | Size: {revenue_cube_kusd.size}\")\n",
    "\n",
    "# Flatten for export to flat-file reporting systems (e.g., CSV)\n",
    "flattened_revenue_export = revenue_cube_kusd.flatten()\n",
    "print(f\" Flattened cube for export: {len(flattened_revenue_export)} values\")\n",
    "print(f\"Flattened Data (first 8): {flattened_revenue_export[:8]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Indexing And Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(1,10)\n",
    "print(s)\n",
    "print(s[1])\n",
    "\n",
    "# lower index (inclusive) to the upper index (exclusive) \n",
    "print(s[1:9])\n",
    "print(s[:5])\n",
    "print(s[5:])\n",
    "\n",
    "row1 = [10,12,13]\n",
    "row2 = [43,32,21]\n",
    "row3 = [65,75,85]\n",
    "nums_2d = np.array([row1, row2, row3])\n",
    "\n",
    "print(nums_2d[:2,:]) # the rows from the first and second index are returned\n",
    "print(nums_2d[:,:2]) # all the rows but only the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated profit margins (%) across 3 regions and 3 products\n",
    "np.random.seed(42)  # For reproducibility\n",
    "profit_matrix = np.random.randn(3, 3) * 15 + 5  # Mean ~5%, std ~15%\n",
    "print(\"\\n Simulated Profit Margins (%):\")\n",
    "print(np.round(profit_matrix, 1))\n",
    "\n",
    "# Extract insights\n",
    "print(\"First region:\", profit_matrix[0])\n",
    "print(\"Best product in Region 1:\", profit_matrix[0, np.argmax(profit_matrix[0])])\n",
    "\n",
    "# Boolean indexing: Find loss-making products\n",
    "loss_makers = profit_matrix < 0\n",
    "print(\"Loss-making cells:\", profit_matrix[loss_makers])\n",
    "\n",
    "## using the `&` and `| `operators\n",
    "# Moderate performers: between -5% and +10%\n",
    "moderate = (profit_matrix > -5) & (profit_matrix < 10)\n",
    "print(\"Moderate performers:\", np.round(profit_matrix[moderate], 1)) \n",
    "\n",
    "# Slicing: Analyze last two regions and first two products\n",
    "subset = profit_matrix[1:, :2]\n",
    "print(\" Subset (Regions 2-3, Products 1-2):\")\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key insights\n",
    "first_region_margins = profit_matrix[0]\n",
    "best_product_in_first_region = profit_matrix[0, np.argmax(profit_matrix[0])]\n",
    "\n",
    "print(f\" Region 1 best product margin: {best_product_in_first_region:.1f}%\")\n",
    "\n",
    "# Boolean indexing: identify loss-making SKUs\n",
    "loss_making_mask = profit_matrix < 0\n",
    "loss_making_values = profit_matrix[loss_making_mask]\n",
    "\n",
    "print(f\" Loss-making SKUs found: {len(loss_making_values)} products\")\n",
    "if len(loss_making_values) > 0:\n",
    "    print(f\" {len(loss_making_values)} products operating at a loss\")\n",
    "\n",
    "# Slice: analyze last two regions and first two products\n",
    "regional_subset = profit_matrix[1:, :2]\n",
    "print(f\"\\n Subset (Regions 2–3, Products 1–2):\\n{regional_subset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic operation with ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY & AGGREGATION\n",
    "print(f\"\\n Profitability Summary Report:\")\n",
    "min_margin = np.round(np.min(profit_matrix), 1)\n",
    "max_margin = np.round(np.max(profit_matrix), 1)\n",
    "avg_margin = np.round(np.mean(profit_matrix), 1)\n",
    "\n",
    "print(f\"• Min Margin: {min_margin}%\")\n",
    "print(f\"• Max Margin: {max_margin}%\")\n",
    "print(f\"• Avg Margin: {avg_margin}%\")\n",
    "\n",
    "# Regional averages (axis=1), Product averages (axis=0)\n",
    "regional_avg_margins = np.round(np.mean(profit_matrix, axis=1), 1)\n",
    "product_avg_margins = np.round(np.mean(profit_matrix, axis=0), 1)\n",
    "\n",
    "print(f\" • By Region: {list(zip(['R1', 'R2', 'R3'], regional_avg_margins))}\")\n",
    "print(f\" • By Product: {list(zip(['P1', 'P2', 'P3'], product_avg_margins))}\")\n",
    "\n",
    "# Identify best-performing region\n",
    "best_region_idx = np.argmax(regional_avg_margins)\n",
    "print(f\"\\n Best Performing Region: R{best_region_idx + 1}\")\n",
    "print(f\" Average Margin: {regional_avg_margins[best_region_idx]:.1f}%\")\n",
    "print(f\" Region {best_region_idx + 1} identified as top performer\")\n",
    "\n",
    "# CUMULATIVE\n",
    "cumulative_margin_by_product = np.cumsum(profit_matrix, axis=1)\n",
    "print(f\"\\n Cumulative Margin by Product (per region):\\n{np.round(cumulative_margin_by_product, 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square roots\n",
    "nums = [100,200,300,400,500]\n",
    "print(np.sqrt(nums))\n",
    "\n",
    "# log\n",
    "print(np.log(nums))\n",
    "\n",
    "# Exponents\n",
    "print(np.exp(nums))\n",
    "\n",
    "# sine and cosine\n",
    "print(np.sin(nums))\n",
    "print(np.cos(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INPUT-OUTPUT TRANSFORMATION MATRICES\n",
    "# Modeling interdependencies between production sectors (e.g., manufacturing, logistics)\n",
    "production_coefficients = np.array([\n",
    "    [2.0, 3.0],  # Manufacturing: raw materials → finished goods\n",
    "    [3.0, 5.0]   # Logistics: distribution scaling with feedback loops\n",
    "], dtype=np.float64)\n",
    "\n",
    "market_response_matrix = np.array([\n",
    "    [1.0, 2.0],  # Consumer demand elasticity\n",
    "    [5.0, -1.0]  # Competitive response (positive & negative shocks)\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(f\" Production Coefficients (A):\\n   {production_coefficients}\")\n",
    "print(f\" Market Response (B):\\n   {market_response_matrix}\\n\")\n",
    "\n",
    "# 2. MATRIX MULTIPLICATION: SYSTEMIC IMPACT MODELING\n",
    "# A × B = Total ripple effect across economic sector\n",
    "total_impact_matrix = np.dot(production_coefficients, market_response_matrix)\n",
    "print(total_impact_matrix)\n",
    "\n",
    "# 3. CROSS PRODUCT: DIRECTIONAL RISK VECTOR ANALYSIS\n",
    "# Measuring orthogonal risk exposure (e.g., portfolio hedging, market momentum)\n",
    "print(\"DIRECTIONAL RISK ASSESSMENT\\n\")\n",
    "\n",
    "cross_ab = np.cross(production_coefficients[:, 0], market_response_matrix[:, 0])\n",
    "cross_ba = np.cross(market_response_matrix[:, 0], production_coefficients[:, 0])\n",
    "\n",
    "print(f\"Risk Vector (A → B): {cross_ab:.1f} (counterclockwise exposure)\")\n",
    "print(f\"Risk Vector (B → A): {cross_ba:.1f} (clockwise exposure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ELEMENT-WISE MULTIPLICATION: DISCOUNT & SCALING FACTORS\n",
    "# Apply personalized pricing or regional adjustments (e.g., promo scaling)\n",
    "pricing_multiplier_grid = np.multiply(production_coefficients, market_response_matrix)\n",
    "\n",
    "print(\" TARGETED DISCOUNT ENGINE (Element-wise Scaling)\\n\")\n",
    "print(f\"Per-Component Pricing Adjustment Matrix (A ⊙ B):\\n{pricing_multiplier_grid}\")\n",
    "\n",
    "# 5. TRANSPOSE: DATA PIVOTING FOR REPORTING\n",
    "# Switch from sector-by-sector to time-series or role-based views\n",
    "production_transposed = np.transpose(production_coefficients)\n",
    "\n",
    "print(\" DATA PIVOT OPERATION (Matrix Transpose)\\n\")\n",
    "print(f\"Original (Rows = Inputs, Cols = Outputs):\\n{production_coefficients}\")\n",
    "print(f\"Transposed (Rows = Outputs, Cols = Inputs):\\n{production_transposed}\")\n",
    "\n",
    "# 6. INVERSE & DETERMINANT: MODEL STABILITY CHECK\n",
    "# Is the economic system reversible? Can we back-solve demand?\n",
    "print(\" MODEL STABILITY DIAGNOSTIC (Invertibility Test)\\n\")\n",
    "\n",
    "production_inverse = np.linalg.inv(production_coefficients)\n",
    "production_determinant = np.linalg.det(production_coefficients)\n",
    "    \n",
    "print(f\" Matrix is invertible. Determinant = {production_determinant:.2f}\")\n",
    "print(f\"Inverse Matrix (A⁻¹) — used for root-cause analysis:\\n{production_inverse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. LINEAR SYSTEM SOLVER: DEMAND BACKCASTING\n",
    "# Given observed outputs, what were the original demand drivers?\n",
    "print(\"LINEAR SYSTEM RESOLUTION: Demand Backcasting\\n\")\n",
    "\n",
    "coeff_matrix = np.array([[2.0, 4.0], [6.0, 8.0]], dtype=np.float64)  # System equations\n",
    "const_vector = np.array([5.0, 6.0])                                 # Observed outcomes\n",
    "\n",
    "solution_vector = np.linalg.solve(coeff_matrix, const_vector)\n",
    "print(f\" System solved. Original demand vector: x = [{solution_vector[0]:.2f}, {solution_vector[1]:.2f}]\")    \n",
    "\n",
    "# 8. EIGEN-DECOMPOSITION: PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "# Identify dominant patterns in multi-dimensional data\n",
    "print(\" PATTERN DETECTION: Eigen-Analysis for Strategic Drivers\\n\")\n",
    "\n",
    "eigen_matrix = np.array([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [1.0, 3.0, 4.0],\n",
    "    [3.0, 2.0, 1.0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(eigen_matrix)\n",
    "\n",
    "print(f\"Top Eigenvalues (Variance explained):\")\n",
    "for i, val in enumerate(eigenvalues):\n",
    "    print(f\" λ{i+1} = {val:.2f}\")\n",
    "\n",
    "print(f\"\\nPrincipal Eigenvector (Strategic Direction):\")\n",
    "print(f\"{eigenvectors[:, 0]}\")\n",
    "\n",
    "# ANALYSIS COMPLETE\n",
    "print(\" OMNIANALYTICS ENGINE: Linear Algebra Module Execution Complete\")\n",
    "print(\" All results ready for export to dashboard, report, or API endpoint.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BROADCASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sales_kusd = np.array([\n",
    "    [110, 120, 130],  # Region 1\n",
    "    [210, 220, 230],  # Region 2\n",
    "    [310, 320, 330]   # Region 3\n",
    "])\n",
    "\n",
    "regional_growth_rates = np.array([1.1, 1.2, 1.05])  # Marketing boost\n",
    "\n",
    "# Broadcasting: apply per-region multiplier across all products\n",
    "projected_sales_kusd = base_sales_kusd * regional_growth_rates[:, np.newaxis]\n",
    "projected_sales_int = projected_sales_kusd.astype(int)\n",
    "\n",
    "print(f\"\\n Projected Sales After Growth (K USD):\\n{projected_sales_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Numbers and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_random = np.random.uniform(4, 5) # uniform distribution\n",
    "normal_random = np.random.randn(4, 5) # normal distribution\n",
    "integer_random = np.random.randint(10, 50, 5) # integers between a certain range\n",
    "\n",
    "np.random.random(10) # random numbers\n",
    "\n",
    "# generate arrays with values from an arbitrary interval [a, b), \n",
    "# where a has to be less than b. (b - a) * random_sample() + a\n",
    "a = -3.4\n",
    "b = 5.9\n",
    "A = (b - a) * np.random.random_sample((3, 4)) + a # Random Samples       \n",
    "\n",
    "# Simulate 4 weeks of daily demand (normal distribution)\n",
    "daily_demand = np.random.randn(4, 5) * 50 + 300  # Mean=300, Std=50\n",
    "print(\"\\n Simulated Daily Demand:\")\n",
    "print(daily_demand.astype(int))\n",
    "\n",
    "# Random order quantities (integer)\n",
    "order_sizes = np.random.randint(20, 100, 5)\n",
    "print(\" Simulated Order Sizes:\", order_sizes)\n",
    "\n",
    "# Sensitivity test: Random inputs in range [-3.4, 5.9]\n",
    "a, b = -3.4, 5.9\n",
    "noise = (b - a) * np.random.random_sample((3, 4)) + a\n",
    "print(\"Sensitivity Noise:\", np.round(noise, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1 = pd.Series(range(1, 6))\n",
    "# The right column contains our `data`, \n",
    "# whereas the left column contains the `index`. \n",
    "\n",
    "#  specify custom index names\n",
    "ser2 = pd.Series(range(1, 6),\n",
    "                 index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "ser2.index\n",
    "ser2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom indexed series for regional risk tiers\n",
    "risk_tier_scores = pd.Series(\n",
    "    data=[1.2, 1.8, 2.1, 1.5, 3.0],\n",
    "    index=['Tier_A', 'Tier_B', 'Tier_C', 'Tier_D', 'Tier_E'],\n",
    "    name=\"Risk_Score_Profile\"\n",
    ")\n",
    "print(risk_tier_scores)\n",
    "print(risk_tier_scores.info())\n",
    "print(risk_tier_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_application_data_raw = {\n",
    "    \"Applicant_Name\": [\n",
    "        \"Jones\", \"Smith\", \"Lynn\", \"Rebecca\", \"Vizon\", \"Phyllis\",\n",
    "        \"Roberto\", \"Sybil\", \"Fernando\", \"Eric\", \"Michael\", \"Fredrick\",\n",
    "        \"Allan\", \"Mary\", \"Joseph\"\n",
    "    ],\n",
    "    \"Loan_Amount_USD\": [\n",
    "        10000, 20000, 1000, 500, 700, 850, 900, 1500, 12000,\n",
    "        16000, 1350, 16000, 8000, 7500, 850\n",
    "    ],\n",
    "    \"Applicant_Age\": [45, 38, 25, 29, 31, 42, 50, 33, 27, 36, 40, 48, 34, 39, 44],\n",
    "    \"Residence_City\": [\n",
    "        \"Taipei\", \"Kaohsiung\", \"Taichung\", \"Hsinchu\", \"Tainan\",\n",
    "        \"Pingtung\", \"Yilan\", \"Hualien\", \"Keelung\", \"Miaoli\",\n",
    "        \"Changhua\", \"Nantou\", \"Yunlin\", \"Chiayi\", \"Taitung\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame — stage 1 raw ingestion\n",
    "df_applications_raw = pd.DataFrame(data=loan_application_data_raw)\n",
    "print(\"\\n Raw Loan Application Data (First 5 Records):\")\n",
    "print(df_applications_raw.head().to_string(index=False)) #head(10) Top 5 rows\n",
    "\n",
    "# Setup the index set.index(column)-- .reset_index() no index\n",
    "df_applications_indexed = df_applications_raw.set_index('Applicant_Name')\n",
    "print(df_applications_indexed.head(3))\n",
    "\n",
    "# EXPLORATORY DATA ANALYSIS (EDA)\n",
    "print(f\"\\n Dataset Shape: {df_applications_indexed.shape}\")\n",
    "print(\"\\n Descriptive Statistics (Numerical Fields):\")\n",
    "print(df_applications_indexed.describe())\n",
    "\n",
    "# For export to CSV (reset index)\n",
    "#df_for_export = df_applications_indexed.reset_index()\n",
    "#export_columns = df_for_export.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection, and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Filtering Rows and Columns\n",
    "# Mock DataFrame for feature engineering\n",
    "feature_matrix = pd.DataFrame({\n",
    "    'Feature_A': range(1, 5),\n",
    "    'Feature_B': range(10, 50, 10),\n",
    "    'Feature_C': range(100, 500, 100),\n",
    "    'Loan_Risk_Score': [800, 1200, 2500, 3100]\n",
    "}, index=['Cust_01', 'Cust_02', 'Cust_03', 'Cust_04'])\n",
    "\n",
    "print(feature_matrix)\n",
    "\n",
    "# Select specific features (by columns)\n",
    "selected_features = feature_matrix[['Feature_A', 'Feature_C']]\n",
    "print(selected_features)\n",
    "\n",
    "# Boolean filtering: high-risk loans only\n",
    "high_risk_loans = feature_matrix[(feature_matrix['Loan_Risk_Score'] > 1000) & \n",
    "                                (feature_matrix['Loan_Risk_Score'] <= 3000)]\n",
    "print(f\"Identified {len(high_risk_loans)} medium-to-high risk applicants\")\n",
    "print(high_risk_loans)\n",
    "\n",
    "# Row access via label (loc) and position (iloc)\n",
    "target_customers = feature_matrix.loc[['Cust_03', 'Cust_04']] # select row(s) by index_name\n",
    "first_applicant = feature_matrix.iloc[0]                      # Select row(s) by index_position\n",
    "print(target_customers)\n",
    "print(first_applicant)\n",
    "\n",
    "# DATA CLEANING & INDEX MANIPULATION\n",
    "feature_matrix_clean = feature_matrix.drop(index='Cust_02') # drop one row by index\n",
    "# feature_matrix_clean = feature_matrix.drop(index='Cust_02', inplace=True) \n",
    "print(\" Removed test record: Cust_02\")\n",
    "print(feature_matrix_clean)\n",
    "\n",
    "# Drop non-critical columns\n",
    "feature_matrix_reduced = feature_matrix_clean.drop(columns=['Feature_A', 'Feature_B']) # by column_name\n",
    "# feature_matrix_clean.drop(['A', 'B'], axis=1)\n",
    "print(feature_matrix_reduced)\n",
    "\n",
    "# Ensure no duplicates exist \n",
    "feature_matrix_unique = feature_matrix_reduced.drop_duplicates()\n",
    "print(\" Deduplication complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting values and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_matrix)\n",
    "# SORTING & RANKING\n",
    "sorted_by_risk = feature_matrix.sort_values(by='Loan_Risk_Score', ascending=False) # by values\n",
    "print(sorted_by_risk)\n",
    "\n",
    "ranked_by_amount = df_applications_indexed[['Loan_Amount_USD']].rank(method='max') # by index\n",
    "print(f\"\\n Ranking by Loan Amount:\\n{ranked_by_amount.head(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(\n",
    "    [[2.4,np.nan],[6.3,-5.4],\n",
    "     [np.nan,np.nan],[0.75,-1.3]],\n",
    "    index=[\"a\",\"b\",\"c\",\"d\"],\n",
    "    columns=[\"one\",\"two\"])\n",
    "\n",
    "df.describe()\n",
    "df.sum()\n",
    "df.sum(axis=1)\n",
    "df.mean(axis=1) \n",
    "\n",
    "df.mean(axis=1,skipna=False)\n",
    "df.idxmax()\n",
    "df.idxmin()\n",
    "df.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging and combining multiple DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MERGING & JOINING\n",
    "# Internal customer tags\n",
    "df_internal = pd.DataFrame({\n",
    "    \"Customer_Tag\": [\"a\", \"b\", \"c\", \"c\", \"d\", \"e\"],\n",
    "    \"Internal_Score\": range(6)\n",
    "})\n",
    "print(df_internal)\n",
    "\n",
    "# External credit bureau match\n",
    "df_external = pd.DataFrame({\n",
    "    \"Bureau_Key\": [\"b\", \"c\", \"e\", \"f\"],\n",
    "    \"Credit_Rating\": range(4)\n",
    "})\n",
    "print(df_external)\n",
    "\n",
    "# Inner join: only matched customers\n",
    "matched_customers = pd.merge(\n",
    "    df_internal, df_external,\n",
    "    left_on=\"Customer_Tag\", right_on=\"Bureau_Key\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(f\" Inner join: {len(matched_customers)} matched customers\")\n",
    "print(matched_customers)\n",
    "\n",
    "# Left join: retain all internal records\n",
    "full_internal_view = pd.merge(df_internal, df_external,\n",
    "                             left_on=\"Customer_Tag\", right_on=\"Bureau_Key\",\n",
    "                             how=\"left\")\n",
    "\n",
    "# Concatenate vertically (new batch)\n",
    "batch_1 = pd.DataFrame({'X': [1, 2], 'Y': [3, 4]}, index=['A', 'C'])\n",
    "batch_2 = pd.DataFrame({'X': [5, 6], 'Y': [7, 8]}, index=['B', 'D'])\n",
    "combined_batches = pd.concat([batch_1, batch_2]).sort_index()\n",
    "print(batch_1)\n",
    "print(batch_2)\n",
    "print(combined_batches)\n",
    "\n",
    "'''\n",
    "# merge by rows\n",
    "pd.concat([df1, df2])\n",
    "pd.concat([df1, df2], axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING & AGGREGATION\n",
    "segmentation_data = pd.DataFrame({\n",
    "    'Customer_Segment': ['Premium', 'Standard', 'Premium', 'Standard', 'Premium', 'Standard'],\n",
    "    'Loan_Value_USD': [10000, 20000, 30000, 40000, 50000, 60000]\n",
    "})\n",
    "print(segmentation_data)\n",
    "\n",
    "grouped_by_segment = segmentation_data.groupby('Customer_Segment')\n",
    "print(grouped_by_segment.describe())\n",
    "\n",
    "mean_per_segment = grouped_by_segment['Loan_Value_USD'].mean()\n",
    "sum_per_segment = grouped_by_segment['Loan_Value_USD'].sum()\n",
    "print(mean_per_segment)\n",
    "print(sum_per_segment)\n",
    "\n",
    "from scipy import stats\n",
    "# Apply z-score normalization within groups\n",
    "normalized_risk = grouped_by_segment['Loan_Value_USD'].transform(stats.zscore)\n",
    "segmentation_data['Risk_Z_Score'] = normalized_risk\n",
    "print(segmentation_data)\n",
    "\n",
    "'''\n",
    "# DataFrameGroupBy.agg(...) accepts functions and \n",
    "# aggregates each column for each group using that method\n",
    "df.groupby('Category').agg({'Value':['sum',,'count']})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLING MISSING DATA\n",
    "df_with_missing = df_applications_indexed.copy() \n",
    "df_with_missing.loc['Jones', 'Applicant_Age'] = np.nan\n",
    "df_with_missing.loc['Smith', 'Loan_Amount_USD'] = np.nan\n",
    "\n",
    "print(f\"\\n  Records with Missing Data:\")\n",
    "print(df_with_missing[df_with_missing.isnull().any(axis=1)])\n",
    "\n",
    "# Cleaning strategy:\n",
    "# - Drop if loan amount missing (critical field)\n",
    "# - Impute age with group mean\n",
    "df_cleaned = df_with_missing.dropna(subset=['Loan_Amount_USD'])\n",
    "mean_age_imputed = df_cleaned['Applicant_Age'].mean()\n",
    "df_cleaned['Applicant_Age'].fillna(mean_age_imputed, inplace=True)\n",
    "\n",
    "print(f\" Imputed missing age with mean value: {mean_age_imputed:.1f}\")\n",
    "print(f\"\\n Cleaned Data Sample:\\n{df_cleaned[['Loan_Amount_USD', 'Applicant_Age']].head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation and and Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMATION & CATEGORIZATION\n",
    "loan_binning_data = pd.DataFrame({\n",
    "    \"Applicant\": [\"Jones\", \"Smith\", \"Lynn\", \"Rebecca\", \"Eric\", \"Fernando\"],\n",
    "    \"Loan_Amount_USD\": [10000, 20000, 1000, 500, 16000, 12000]\n",
    "})\n",
    "\n",
    "# Equal-width bins\n",
    "min_loan = loan_binning_data['Loan_Amount_USD'].min()\n",
    "max_loan = loan_binning_data['Loan_Amount_USD'].max()\n",
    "bins = np.linspace(min_loan, max_loan, num=4)  # 3 groups\n",
    "labels = [\"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "loan_binning_data[\"Risk_Tier_EqualWidth\"] = pd.cut(\n",
    "    loan_binning_data[\"Loan_Amount_USD\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Equal-frequency bins (quartiles)\n",
    "loan_binning_data[\"Risk_Tier_EqualFreq\"] = pd.qcut(\n",
    "    loan_binning_data[\"Loan_Amount_USD\"],\n",
    "    q=3,\n",
    "    precision=1,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "print(f\"\\n Loan Tier Distribution (Equal Frequency):\\n{loan_binning_data['Risk_Tier_EqualFreq'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGION MAPPING & PIVOT REPORTING\n",
    "region_mapping = {\n",
    "    'Taipei': 'North', 'Kaohsiung': 'South', 'Taichung': 'Central',\n",
    "    'Hsinchu': 'North', 'Tainan': 'South', 'Pingtung': 'South',\n",
    "    'Yilan': 'North', 'Hualien': 'East', 'Keelung': 'North',\n",
    "    'Miaoli': 'North', 'Changhua': 'Central', 'Nantou': 'Central',\n",
    "    'Yunlin': 'Central', 'Chiayi': 'South', 'Taitung': 'East'\n",
    "}\n",
    "\n",
    "df_final = df_cleaned.reset_index().copy()\n",
    "df_final['Region'] = df_final['Residence_City'].map(region_mapping)\n",
    "df_final['Risk_Group'] = np.where(df_final['Loan_Amount_USD'] > 10000, 'High_Value', 'Standard')\n",
    "\n",
    "# Pivot: Regional exposure by risk group\n",
    "pivot_loan_volume = pd.pivot_table(\n",
    "    df_final,\n",
    "    index='Region',\n",
    "    columns='Risk_Group',\n",
    "    values='Loan_Amount_USD',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").astype(int)\n",
    "\n",
    "print(\" Generated pivot table for regional risk exposure\")\n",
    "print(f\"\\n Loan Volume by Region & Risk Group (USD):\\n{pivot_loan_volume}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Array and Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARRAY ↔ DATAFRAME INTEROPERABILITY\n",
    "raw_numerical_data = np.array([[11, 22, 33], [44, 55, 66]])\n",
    "df_from_array = pd.DataFrame(\n",
    "    raw_numerical_data,\n",
    "    columns=['Feature_1', 'Feature_2', 'Feature_3']\n",
    ")\n",
    "print(\"Converted NumPy array to Pandas DataFrame for feature engineering\")\n",
    "\n",
    "# Reverse: export to NumPy for model training\n",
    "numerical_df = pd.DataFrame({\n",
    "    'Age': [25, 47, 38],\n",
    "    'Birth_Year': [1995, 1973, 1982],\n",
    "    'Graduation_Year': [2016, 2000, 2005]\n",
    "})\n",
    "model_input_array = numerical_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample real-world data (e.g., from a CSV or database)\n",
    "data = {\n",
    "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005'],\n",
    "    'annual_spending': [12000, 8000, 15000, 3000, 500],\n",
    "    'purchase_frequency': [5, 3, 6, 1, 0.5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Simple function to categorize customer value\n",
    "def assign_tier(row):\n",
    "    if row['annual_spending'] > 10000 and row['purchase_frequency'] >= 4:\n",
    "        return 'Premium'\n",
    "    elif row['annual_spending'] > 5000 and row['purchase_frequency'] >= 2:\n",
    "        return 'Standard'\n",
    "    else:\n",
    "        return 'Basic'\n",
    "\n",
    "# Step 3: Use apply() to create a new column\n",
    "df['customer_tier'] = df.apply(assign_tier, axis=1)\n",
    "\n",
    "# Step 4: Show result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')  # Clean, professional look\n",
    "plt.rcParams['figure.figsize'] = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Trend Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot monthly sales performance with markers and styling\n",
    "months_numeric = np.array([1, 2, 3, 4, 5, 6])\n",
    "actual_sales_kusd = np.array([120, 135, 140, 130, 145, 160])\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(months_numeric, actual_sales_kusd,\n",
    "         marker='o', ms=8, mec='darkblue', color='green',\n",
    "         linestyle='-', linewidth=2, label='Actual Sales') # <--- plot\n",
    "\n",
    "# Inline set_title logic: set title and subtitle\n",
    "plt.title(\"Monthly Sales Trend (2025)\\nJan–Jun Performance\", fontweight='bold')\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sales (K USD)\")\n",
    "plt.xticks(months_numeric, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"sales_trend_2025.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCATTER PLOT: AD SPEND VS. SALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing ROI scatter plot\n",
    "ad_spend_kusd = np.array([50, 60, 70, 80, 90, 100])\n",
    "sales_revenue_kusd = np.array([120, 130, 145, 150, 160, 175])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(ad_spend_kusd, sales_revenue_kusd,\n",
    "            color='#2ca02c', alpha=0.7, s=60) # <--- scatter\n",
    "            \n",
    "plt.title(\"Advertising Spend vs. Sales Performance\", fontweight='bold')\n",
    "plt.xlabel(\"Ad Spend (K USD)\")\n",
    "plt.ylabel(\"Sales Revenue (K USD)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"ad_spend_vs_sales.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BAR CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product revenue bar chart\n",
    "products = [\"Laptops\", \"Phones\", \"Tablets\", \"Accessories\"]\n",
    "revenue_kusd = [240, 180, 90, 70]\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(products, revenue_kusd, color=colors) # <--- bar\n",
    "# Inline set_title logic: set title\n",
    "plt.title(\"Revenue by Product Category\", fontweight='bold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 5,\n",
    "             f'{int(yval)}K', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Revenue (K USD)\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"revenue_by_product.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HORIZONTAL BAR\n",
    "# store performance horizontal bar chart\n",
    "stores = [\"Taipei\", \"Kaohsiung\", \"Taichung\", \"Hsinchu\", \"Tainan\"]\n",
    "performance_index = [95, 88, 92, 85, 80]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(stores, performance_index, color='steelblue') # <---- barh\n",
    "\n",
    "# Inline set_title logic: set title and subtitle\n",
    "plt.title(\"Store Performance Score\\nQ1 2025 Review\", fontweight='bold')\n",
    "plt.xlabel(\"Performance Index (0–100)\")\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"store_performance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTOGRAM\n",
    "# customer age histogram\n",
    "np.random.seed(42)\n",
    "customer_ages = np.random.normal(38, 10, 500).clip(18, 70)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(customer_ages, bins=20, color='teal', alpha=0.7, edgecolor='black') # <--- hist\n",
    "\n",
    "# Inline set_title logic: set title and subtitle\n",
    "plt.title(\"Customer Age Distribution\\nn=500\", fontweight='bold')\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"age_distribution.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIE CHART\n",
    "# market share pie chart\n",
    "regions = [\"North\", \"South\", \"Central\", \"East\"]\n",
    "market_share_pct = [40, 30, 20, 10]\n",
    "explode = [0.1, 0, 0, 0]  # Highlight North\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(market_share_pct, labels=regions, autopct='%1.1f%%',\n",
    "        startangle=90, explode=explode, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99']) # <--- pie\n",
    "\n",
    "# Inline set_title logic: set title and subtitle\n",
    "plt.title(\"Market Share by Region\\nNorth region receives strategic focus\", fontweight='bold')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"market_share.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARATIVE LINE PLOT\n",
    "# actual vs. target performance chart\n",
    "months = np.arange(1, 7)\n",
    "actual_sales = np.array([120, 135, 140, 130, 145, 160])\n",
    "target_sales = np.array([125, 130, 135, 140, 145, 150])\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(months, actual_sales, marker='o', color='blue',\n",
    "         label='Actual Sales', linewidth=2) # <--- plot 1\n",
    "plt.plot(months, target_sales, linestyle='--', marker='s', color='red',\n",
    "         label='Sales Target', linewidth=2) # <--- plot 2\n",
    "\n",
    "# Inline set_title logic: set title and subtitle\n",
    "plt.title(\"Actual vs. Target Sales\\nPerformance Gap Analysis\", fontweight='bold')\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Sales (K USD)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(months, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"actual_vs_target.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBPLOTS\n",
    "np.random.seed(123)\n",
    "months = np.arange(1, 7)\n",
    "actual_sales = np.array([120, 135, 140, 130, 145, 160])\n",
    "\n",
    "products = [\"Laptops\", \"Phones\", \"Tablets\", \"Accessories\"]\n",
    "revenue = [240, 180, 90, 70]\n",
    "\n",
    "customer_ages = np.random.normal(38, 10, 500).clip(18, 70)\n",
    "\n",
    "regions = [\"North\", \"South\", \"Central\", \"East\"]\n",
    "market_share = [40, 30, 20, 10]\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Business Intelligence Dashboard — Q2 2025\", fontsize=16, fontweight='bold') #<---suptitle\n",
    "\n",
    "# 1. Sales Trend\n",
    "ax1.plot(months, actual_sales, marker='o', color='blue') # <---plot\n",
    "ax1.set_title(\"Sales Trend\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylabel(\"Sales (K USD)\")\n",
    "ax1.set_xticks(months)\n",
    "ax1.set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'], rotation=15)\n",
    "\n",
    "# 2. Product Revenue\n",
    "ax2.bar(products, revenue, color='skyblue') # <--- bar\n",
    "ax2.set_title(\"Revenue by Product\")\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# 3. Age Distribution\n",
    "ax3.hist(customer_ages, bins=15, color='teal', alpha=0.7) #<--- hist\n",
    "ax3.set_title(\"Customer Age Distribution\")\n",
    "ax3.set_xlabel(\"Age\")\n",
    "ax3.set_ylabel(\"Count\")\n",
    "\n",
    "# 4. Market Share\n",
    "ax4.pie(market_share, labels=regions, autopct='%1.1f%%', startangle=90,\n",
    "        colors=['#ff9999','#66b3ff','#99ff99','#ffcc99']) # <-- pie\n",
    "ax4.set_title(\"Market Share\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.savefig(\"executive_dashboard_q1_2025.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME SERIES\n",
    "# trend area plot\n",
    "dates = pd.date_range('2021-01-01', periods=60, freq='M')\n",
    "passengers = np.random.randint(300, 500, size=60).cumsum()\n",
    "\n",
    "df_flights = pd.DataFrame({'Passengers': passengers}, index=dates)\n",
    "\n",
    "df_flights.plot.area(y='Passengers', figsize=(9, 5),\n",
    "                     title=\"Cumulative Passengers Over Time\",\n",
    "                     color='orange', alpha=0.7) # <--- area\n",
    "plt.ylabel(\"Total Passengers\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"passenger_growth_area.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOX PLOT\n",
    "# box plot for daily sales variability\"\n",
    "np.random.seed(42)\n",
    "tip_data = pd.DataFrame({\n",
    "    'Day': np.random.choice(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], 200),\n",
    "    'Total_Bill_USD': np.random.lognormal(3, 0.5, 200) * 10\n",
    "})\n",
    "\n",
    "tip_data['Day'] = pd.Categorical(tip_data['Day'],\n",
    "                                 categories=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'],\n",
    "                                 ordered=True)\n",
    "\n",
    "tip_data.boxplot(by='Day', column=['Total_Bill_USD'],\n",
    "                 vert=False, grid=False, figsize=(9, 5)) # <--- boxplot\n",
    "plt.title(\"Sales Distribution by Day of Week\")\n",
    "plt.suptitle(\"\")  # Remove default\n",
    "plt.xlabel(\"Total Bill (USD)\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"sales_by_day_boxplot.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPED BAR\n",
    "# average age by gender bar chart\n",
    "np.random.seed(123)\n",
    "df_customers = pd.DataFrame({\n",
    "    'Gender': np.random.choice(['Female', 'Male'], 100),\n",
    "    'Age': np.random.randint(18, 70, 100)\n",
    "})\n",
    "\n",
    "avg_age_by_gender = df_customers.groupby('Gender')['Age'].mean().reindex(['Female', 'Male'])\n",
    "\n",
    "df_gender = pd.DataFrame({\n",
    "    'Gender': avg_age_by_gender.index,\n",
    "    'Average_Age': avg_age_by_gender.values\n",
    "})\n",
    "\n",
    "ax = df_gender.plot.bar(x='Gender', y='Average_Age', figsize=(8, 5),\n",
    "                        title=\"Average Customer Age by Gender\",\n",
    "                        legend=False, color=['pink', 'lightblue']) # plot.bar\n",
    "ax.set_ylabel(\"Age\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"avg_age_by_gender.png\", dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
